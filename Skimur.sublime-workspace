{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"m",
				"msys"
			]
		]
	},
	"buffers":
	[
		{
			"file": "/D/Git/reddit/r2/r2/lib/normalized_hot.py",
			"settings":
			{
				"buffer_size": 3817,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/menus.py",
			"settings":
			{
				"buffer_size": 25179,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/controllers/wiki.py",
			"settings":
			{
				"buffer_size": 22249,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/db/sorts.py",
			"settings":
			{
				"buffer_size": 1242,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/db/_sorts.pyx",
			"settings":
			{
				"buffer_size": 4556,
				"line_ending": "Windows"
			}
		},
		{
			"contents": "Searching 5266 files for \"create index\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\db\\alter_db.py:\n   64  \"drop index idx_thing_id_%(type)s\"\n   65  \n   66: \"create index concurrently idx_thing1_name_date_%(type)s on %(type)s (thing1_id, name, date);\"\n   67  \n\nD:\\Git\\reddit\\r2\\r2\\lib\\db\\tdb_lite.py:\n   34  \n   35      def index_str(self, table, name, on, where = None):\n   36:         index_str = 'create index idx_%s_' % name\n   37          index_str += table.name\n   38          index_str += ' on '+ table.name + ' (%s)' % on\n\nD:\\Git\\reddit\\r2\\r2\\lib\\db\\tdb_sql.py:\n  127          index_str = 'create unique index'\n  128      else:\n  129:         index_str = 'create index'\n  130      index_str += ' idx_%s_' % name\n  131      index_str += table.name\n\n3 matches across 3 files\n\n\nSearching 5266 files for \"create index\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\db\\alter_db.py:\n   64  \"drop index idx_thing_id_%(type)s\"\n   65  \n   66: \"create index concurrently idx_thing1_name_date_%(type)s on %(type)s (thing1_id, name, date);\"\n   67  \n\nD:\\Git\\reddit\\r2\\r2\\lib\\db\\tdb_lite.py:\n   34  \n   35      def index_str(self, table, name, on, where = None):\n   36:         index_str = 'create index idx_%s_' % name\n   37          index_str += table.name\n   38          index_str += ' on '+ table.name + ' (%s)' % on\n\nD:\\Git\\reddit\\r2\\r2\\lib\\db\\tdb_sql.py:\n  127          index_str = 'create unique index'\n  128      else:\n  129:         index_str = 'create index'\n  130      index_str += ' idx_%s_' % name\n  131      index_str += table.name\n\n3 matches across 3 files\n\n\nSearching 5266 files for \"rising\" (case sensitive)\n\nD:\\Git\\reddit\\install-reddit.sh:\n  763  */5  * * * * root /sbin/start --quiet reddit-job-clean_up_hardcache\n  764  */2  * * * * root /sbin/start --quiet reddit-job-broken_things\n  765: */2  * * * * root /sbin/start --quiet reddit-job-rising\n  766  0    * * * * root /sbin/start --quiet reddit-job-trylater\n  767  \n\nD:\\Git\\reddit\\LICENSE:\n  395  \n  396  As between Initial Developer, Original Developer and the Contributors, each\n  397: party is responsible for claims and damages arising, directly or indirectly, out\n  398  of its utilization of rights under this License and You agree to work with\n  399  Initial Developer, Original Developer and Contributors to distribute such\n\nD:\\Git\\reddit\\r2\\example.ini:\n  398  # maximum age (in days) of items eligible for display on normalized hot pages (frontpage, multis, etc.)\n  399  HOT_PAGE_AGE = 1000\n  400: # how long to consider links eligible for the rising page\n  401: rising_period = 12 hours\n  402  # default number of comments shown\n  403  num_comments = 100\n\nD:\\Git\\reddit\\r2\\r2\\config\\routing.py:\n  160            action='listing', requirements=dict(sort='top|controversial'))\n  161         connect('/:controller', action='listing',\n  162:           requirements=dict(controller=\"hot|new|rising|randomrising|ads\"))\n  163  \n  164      mc('/user/:username/:where/:show', controller='user', action='listing')\n  ...\n  274  \n  275      mc('/:controller', action='listing',\n  276:        requirements=dict(controller=\"hot|new|rising|randomrising|ads\"))\n  277      mc('/saved', controller='user', action='saved_redirect')\n  278  \n\nD:\\Git\\reddit\\r2\\r2\\controllers\\__init__.py:\n   50      from listingcontroller import RedditsController\n   51      from listingcontroller import ByIDController\n   52:     from listingcontroller import RandomrisingController\n   53      from listingcontroller import UserController\n   54      from listingcontroller import CommentsController\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\listingcontroller.py:\n   36  from r2.lib.menus import TimeMenu, SortMenu, RecSortMenu, ProfileSortMenu\n   37  from r2.lib.menus import ControversyTimeMenu, menu, QueryButton\n   38: from r2.lib.rising import get_rising, normalized_rising\n   39  from r2.lib.wrapped import Wrapped\n   40  from r2.lib.normalized_hot import normalized_hot\n   ..\n  524      @listing_api_doc(uri='/new', uses_site=True)\n  525      def GET_listing(self, **env):\n  526:         if request.params.get('sort') == 'rising':\n  527:             return self.redirect(add_sr('/rising'))\n  528  \n  529          return ListingController.GET_listing(self, **env)\n  530  \n  531  class RisingController(NewController):\n  532:     where = 'rising'\n  533:     title_text = _('rising submissions')\n  534:     extra_page_classes = ListingController.extra_page_classes + ['rising-page']\n  535  \n  536      def query(self):\n  537          if isinstance(c.site, DefaultSR):\n  538              sr_ids = Subreddit.user_subreddits(c.user)\n  539:             return normalized_rising(sr_ids)\n  540          elif isinstance(c.site, MultiReddit):\n  541:             return normalized_rising(c.site.kept_sr_ids)\n  542  \n  543:         return get_rising(c.site)\n  544  \n  545  class BrowseController(ListingWithPromos):\n  ...\n  637  \n  638  \n  639: class RandomrisingController(ListingWithPromos):\n  640:     where = 'randomrising'\n  641      title_text = _('you\\'re really bored now, eh?')\n  642      next_suggestions_cls = ListingSuggestions\n  643  \n  644      def query(self):\n  645:         links = get_rising(c.site)\n  646  \n  647          if not links:\n  648:             # just pull from the new page if the rising page isn't\n  649              # populated for some reason\n  650              links = c.site.get_links('new', 'all')\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\post.py:\n  158          personalized=VBoolean('pers', default=False),\n  159          discovery=VBoolean('disc', default=False),\n  160:         rising=VBoolean('ris', default=False),\n  161          nsfw=VBoolean('nsfw', default=False),\n  162      )\n  ...\n  166                                personalized,\n  167                                discovery,\n  168:                               rising,\n  169                                nsfw):\n  170          ExploreSettings.record_settings(\n  ...\n  172              personalized=personalized,\n  173              discovery=discovery,\n  174:             rising=rising,\n  175              nsfw=nsfw,\n  176          )\n\nD:\\Git\\reddit\\r2\\r2\\lib\\count.py:\n   26  from pylons import g, config\n   27  \n   28: count_period = g.rising_period\n   29  \n   30  #stubs\n\nD:\\Git\\reddit\\r2\\r2\\lib\\menus.py:\n   60                       saved        = _('saved {toolbar}'),\n   61                       recommended  = _('recommended'),\n   62:                      rising       = _('rising'), \n   63                       admin        = _('admin'), \n   64                                   \n\nD:\\Git\\reddit\\r2\\r2\\lib\\recommender.py:\n   28  from operator import itemgetter\n   29  \n   30: from r2.lib import rising\n   31  from r2.lib.db import operators, tdb_cassandra\n   32  from r2.lib.pages import ExploreItem\n   ..\n   51  \n   52  # explore item types\n   53: TYPE_RISING = _(\"rising\")\n   54  TYPE_DISCOVERY = _(\"discovery\")\n   55  TYPE_HOT = _(\"hot\")\n   ..\n  133      num_recs = 20  # how many recommended srs to ask for\n  134      num_discovery = 2  # how many discovery-related subreddits to mix in\n  135:     num_rising = 4  # how many rising links to mix in\n  136      num_items = 20  # total items to return\n  137:     rising_items = discovery_items = comment_items = hot_items = []\n  138  \n  139      # make a list of srs that shouldn't be recommended\n  ...\n  172          # get links from subreddits dedicated to discovery\n  173          discovery_items = get_hot_items(discovery_srs, TYPE_DISCOVERY, 'disc')\n  174:     if settings.rising:\n  175:         # grab some (non-personalized) rising items\n  176          omit_sr_ids = set(int(id36, 36) for id36 in omit_srid36s)\n  177:         rising_items = get_rising_items(omit_sr_ids, count=num_rising)\n  178      # combine all items and randomize order to get a mix of types\n  179:     all_recs = list(chain(rising_items,\n  180                            comment_items,\n  181                            discovery_items,\n  ...\n  188          if not settings.nsfw and r.is_over18():\n  189              continue\n  190:         if not is_visible(r.sr):  # could happen in rising items\n  191              continue\n  192          if r.sr._id not in seen_srs:\n  ...\n  209  \n  210  \n  211: def get_rising_items(omit_sr_ids, count=4):\n  212:     \"\"\"Get links that are rising right now.\"\"\"\n  213:     all_rising = rising.get_all_rising()\n  214:     candidate_sr_ids = {sr_id for link, score, sr_id in all_rising}.difference(omit_sr_ids)\n  215:     link_fullnames = [link for link, score, sr_id in all_rising if sr_id in candidate_sr_ids]\n  216      link_fullnames_to_show = random_sample(link_fullnames, count)\n  217:     rising_links = Link._by_fullname(link_fullnames_to_show,\n  218                                       return_dict=False,\n  219                                       data=True)\n  220:     rising_items = [ExploreItem(TYPE_RISING, 'ris', Subreddit._byID(l.sr_id), l)\n  221:                    for l in rising_links]\n  222:     return rising_items\n  223  \n  224  \n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   31  \n   32  \n   33: CACHE_KEY = \"rising\"\n   34  \n   35  \n   36: def calc_rising():\n   37      link_counts = count.get_link_counts()\n   38  \n   ..\n   43          return float(link._ups) / max(count, 1)\n   44  \n   45:     # build the rising list, excluding items having 1 or less upvotes\n   46:     rising = []\n   47      for link in links.values():\n   48          if link._ups > 1:\n   49:             rising.append((link._fullname, score(link), link.sr_id))\n   50  \n   51:     # return rising sorted by score\n   52:     return sorted(rising, key=lambda x: x[1], reverse=True)\n   53  \n   54  \n   55: def set_rising():\n   56:     g.cache.set(CACHE_KEY, calc_rising())\n   57  \n   58  \n   59: def get_all_rising():\n   60      return g.cache.get(CACHE_KEY, [])\n   61  \n   62  \n   63: def get_rising(sr):\n   64:     rising = get_all_rising()\n   65:     return [link for link, score, sr_id in rising if sr.keep_for_rising(sr_id)]\n   66  \n   67  \n   68: def get_rising_tuples(sr_ids):\n   69:     rising = get_all_rising()\n   70  \n   71      tuples_by_srid = {sr_id: [] for sr_id in sr_ids}\n   72:     top_rising = {}\n   73  \n   74:     for link, score, sr_id in rising:\n   75          if sr_id not in sr_ids:\n   76              continue\n   77  \n   78:         if sr_id not in top_rising:\n   79:             top_rising[sr_id] = score\n   80  \n   81:         norm_score = score / top_rising[sr_id]\n   82          tuples_by_srid[sr_id].append((-norm_score, -score, link))\n   83  \n   ..\n   85  \n   86  \n   87: def normalized_rising(sr_ids):\n   88      if not sr_ids:\n   89          return []\n   90  \n   91:     tuples_by_srid = sgm(g.cache, sr_ids, miss_fn=get_rising_tuples,\n   92:                          prefix='normalized_rising', time=g.page_cache_time)\n   93  \n   94      merged = heapq.merge(*tuples_by_srid.values())\n\nD:\\Git\\reddit\\r2\\r2\\lib\\wrapped.pyx:\n  499          else:\n  500              self.render_class = self.__class__\n  501:         # this shouldn't be too surprising\n  502          self.cache_ignore = self.cache_ignore.union(\n  503              set(['cachable', 'render', 'cache_ignore', 'lookups']))\n\nD:\\Git\\reddit\\r2\\r2\\lib\\pages\\pages.py:\n  801              main_buttons = [NamedButton('hot', dest='', aliases=['/hot']),\n  802                              NamedButton('new'),\n  803:                             NamedButton('rising'),\n  804                              NamedButton('controversial'),\n  805                              NamedButton('top'),\n\nD:\\Git\\reddit\\r2\\r2\\models\\recommend.py:\n  133      \"\"\"Column family for storing users' view prefs for the /explore page.\"\"\"\n  134      _use_db = True\n  135:     _bool_props = ('personalized', 'discovery', 'rising', 'nsfw')\n  136  \n  137      @classmethod\n  ...\n  148                          personalized=False,\n  149                          discovery=False,\n  150:                         rising=False,\n  151                          nsfw=False):\n  152          \"\"\"Update or create settings for user.\"\"\"\n  ...\n  158                  personalized=personalized,\n  159                  discovery=discovery,\n  160:                 rising=rising,\n  161                  nsfw=nsfw,\n  162              )\n  ...\n  164              settings.personalized = personalized\n  165              settings.discovery = discovery\n  166:             settings.rising = rising\n  167              settings.nsfw = nsfw\n  168          settings._commit()\n  ...\n  174          self.personalized = True\n  175          self.discovery = True\n  176:         self.rising = True\n  177          self.nsfw = False\n  178  \n\nD:\\Git\\reddit\\r2\\r2\\models\\subreddit.py:\n  820          return subreddits if return_dict else subreddits.values()\n  821  \n  822:     def keep_for_rising(self, sr_id):\n  823:         \"\"\"Return whether or not to keep a thing in rising for this SR.\"\"\"\n  824          return sr_id == self._id\n  825  \n  ...\n 1143          BaseSite.__init__(self)\n 1144  \n 1145:     def keep_for_rising(self, sr_id):\n 1146          return False\n 1147  \n ....\n 1266      path = '/r/all'\n 1267  \n 1268:     def keep_for_rising(self, sr_id):\n 1269          return True\n 1270  \n ....\n 1302          self.exclude_sr_ids = [sr._id for sr in srs]\n 1303  \n 1304:     def keep_for_rising(self, sr_id):\n 1305          return sr_id not in self.exclude_sr_ids\n 1306  \n ....\n 1372          return c.defaultsr_cached_sr_ids\n 1373  \n 1374:     def keep_for_rising(self, sr_id):\n 1375          return sr_id in self._get_sr_ids()\n 1376  \n ....\n 1521          return all(sr.allows_referrers for sr in self.srs)\n 1522  \n 1523:     def keep_for_rising(self, sr_id):\n 1524          return sr_id in self.kept_sr_ids\n 1525  \n\nD:\\Git\\reddit\\r2\\r2\\public\\static\\css\\reddit.less:\n 1224  }\n 1225  \n 1226: .explore-rising .explore-label {\n 1227      background-color: #d6fbcb;\n 1228      border: solid thin #485;\n\nD:\\Git\\reddit\\r2\\r2\\public\\static\\js\\lib\\react-with-addons-0.11.2.js:\n 16454      // The actual meaning of the value depends on the users' keyboard layout\n 16455      // which cannot be detected. Assuming that it is a US keyboard layout\n 16456:     // provides a surprisingly accurate mapping for US and European users.\n 16457      // Due to this, it is left to the user to implement at this time.\n 16458      if (event.type === 'keydown' || event.type === 'keyup') {\n\nD:\\Git\\reddit\\r2\\r2\\templates\\exploreitemlisting.html:\n   42              ${_(\"discovery\")}\n   43            </input>\n   44:           <input type=\"checkbox\" name=\"ris\" value=1 ${\"checked\" if thing.settings.rising else \"\"}>\n   45:             ${_(\"rising\")}\n   46            </input>\n   47            <input type=\"checkbox\" name=\"nsfw\" value=1 ${\"checked\" if thing.settings.nsfw else \"\"}>\n\nD:\\Git\\reddit\\upstart\\reddit-job-rising.conf:\n    1: description \"update the rising pages\"\n    2  \n    3  task\n    .\n    9  script\n   10      . /etc/default/reddit\n   11:     wrap-job paster run $REDDIT_INI -c 'from r2.lib import rising; rising.set_rising()'\n   12  end script\n   13  \n\nD:\\Git\\Skimur\\LICENSE:\n  157  12.	RESPONSIBILITY FOR CLAIMS.\n  158  \n  159: As between Initial Developer, Original Developer and the Contributors, each party is responsible for claims and damages arising, directly or indirectly, out of its utilization of rights under this License and You agree to work with Initial Developer, Original Developer and Contributors to distribute such responsibility on an equitable basis. Nothing herein is intended or shall be deemed to constitute any admission of liability.\n  160  \n  161  13.	MULTIPLE-LICENSED CODE.\n\nD:\\Git\\Skimur\\puppet\\modules\\postgresql\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\puppet\\modules\\rabbitmq\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\puppet\\modules\\redis\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\packages\\NUnit.2.6.4\\license.txt:\n    3  Copyright © 2000-2002 Philip A. Craig\n    4  \n    5: This software is provided 'as-is', without any express or implied warranty. In no event will the authors be held liable for any damages arising from the use of this software.\n    6  \n    7  Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions:\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-datepicker\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-datetimepicker\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-modal\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-switch\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\ckeditor\\LICENSE.md:\n 1214  \n 1215       As between Initial Developer and the Contributors, each party is\n 1216:      responsible for claims and damages arising, directly or indirectly,\n 1217       out of its utilization of rights under this License and You agree to\n 1218       work with Initial Developer and Contributors to distribute such\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\examples\\advanced_init\\dt_events.html:\n   49  \n   50  				<p>All custom events fired by DataTables are fired with the namespace <code>dt</code> in order to\n   51: 				prevent conflicts arising with other jQuery plug-ins which also fire events. The DataTables <a href=\n   52  				\"//datatables.net/reference/api/on()\"><code class=\"api\" title=\n   53  				\"DataTables API method\">on()<span>DT</span></code></a> method can be used like the jQuery\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\examples\\advanced_init\\footer_callback.html:\n   74  				\"//datatables.net/reference/option/footerCallback\"><code class=\"option\" title=\n   75  				\"DataTables initialisation option\">footerCallback<span>DT</span></code></a>), it is possible to perform\n   76: 				some powerful and useful data manipulation functions, such as summarising data in the table.</p>\n   77  \n   78  				<p>The example below shows a footer callback being used to total the data for a column (both the\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\media\\js\\jquery.dataTables.js:\n 6640  		 * @param {*} value1 Arrays and/or values to concatenate.\n 6641  		 * @param {*} [...] Additional arrays and/or values to concatenate.\n 6642: 		 * @returns {DataTables.Api} New API instance, comprising of the combined\n 6643  		 *   array.\n 6644  		 */\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\fullcalendar\\fullcalendar.js:\n 4784  \n 4785  	rowEls: null, // set of fake row elements\n 4786: 	dayEls: null, // set of whole-day elements comprising the row's background\n 4787  	helperEls: null, // set of cell skeleton elements for rendering the mock event \"helper\"\n 4788  	highlightEls: null, // set of cell skeleton elements for rendering the highlight\n\n127 matches across 42 files\n\n\nSearching 5266 files for \"reddit-job-rising\" (case sensitive)\n\nD:\\Git\\reddit\\install-reddit.sh:\n  763  */5  * * * * root /sbin/start --quiet reddit-job-clean_up_hardcache\n  764  */2  * * * * root /sbin/start --quiet reddit-job-broken_things\n  765: */2  * * * * root /sbin/start --quiet reddit-job-rising\n  766  0    * * * * root /sbin/start --quiet reddit-job-trylater\n  767  \n\n1 match in 1 file\n\n\nSearching 5266 files for \"rising\" (case sensitive)\n\nD:\\Git\\reddit\\install-reddit.sh:\n  763  */5  * * * * root /sbin/start --quiet reddit-job-clean_up_hardcache\n  764  */2  * * * * root /sbin/start --quiet reddit-job-broken_things\n  765: */2  * * * * root /sbin/start --quiet reddit-job-rising\n  766  0    * * * * root /sbin/start --quiet reddit-job-trylater\n  767  \n\nD:\\Git\\reddit\\LICENSE:\n  395  \n  396  As between Initial Developer, Original Developer and the Contributors, each\n  397: party is responsible for claims and damages arising, directly or indirectly, out\n  398  of its utilization of rights under this License and You agree to work with\n  399  Initial Developer, Original Developer and Contributors to distribute such\n\nD:\\Git\\reddit\\r2\\example.ini:\n  398  # maximum age (in days) of items eligible for display on normalized hot pages (frontpage, multis, etc.)\n  399  HOT_PAGE_AGE = 1000\n  400: # how long to consider links eligible for the rising page\n  401: rising_period = 12 hours\n  402  # default number of comments shown\n  403  num_comments = 100\n\nD:\\Git\\reddit\\r2\\r2\\config\\routing.py:\n  160            action='listing', requirements=dict(sort='top|controversial'))\n  161         connect('/:controller', action='listing',\n  162:           requirements=dict(controller=\"hot|new|rising|randomrising|ads\"))\n  163  \n  164      mc('/user/:username/:where/:show', controller='user', action='listing')\n  ...\n  274  \n  275      mc('/:controller', action='listing',\n  276:        requirements=dict(controller=\"hot|new|rising|randomrising|ads\"))\n  277      mc('/saved', controller='user', action='saved_redirect')\n  278  \n\nD:\\Git\\reddit\\r2\\r2\\controllers\\__init__.py:\n   50      from listingcontroller import RedditsController\n   51      from listingcontroller import ByIDController\n   52:     from listingcontroller import RandomrisingController\n   53      from listingcontroller import UserController\n   54      from listingcontroller import CommentsController\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\listingcontroller.py:\n   36  from r2.lib.menus import TimeMenu, SortMenu, RecSortMenu, ProfileSortMenu\n   37  from r2.lib.menus import ControversyTimeMenu, menu, QueryButton\n   38: from r2.lib.rising import get_rising, normalized_rising\n   39  from r2.lib.wrapped import Wrapped\n   40  from r2.lib.normalized_hot import normalized_hot\n   ..\n  524      @listing_api_doc(uri='/new', uses_site=True)\n  525      def GET_listing(self, **env):\n  526:         if request.params.get('sort') == 'rising':\n  527:             return self.redirect(add_sr('/rising'))\n  528  \n  529          return ListingController.GET_listing(self, **env)\n  530  \n  531  class RisingController(NewController):\n  532:     where = 'rising'\n  533:     title_text = _('rising submissions')\n  534:     extra_page_classes = ListingController.extra_page_classes + ['rising-page']\n  535  \n  536      def query(self):\n  537          if isinstance(c.site, DefaultSR):\n  538              sr_ids = Subreddit.user_subreddits(c.user)\n  539:             return normalized_rising(sr_ids)\n  540          elif isinstance(c.site, MultiReddit):\n  541:             return normalized_rising(c.site.kept_sr_ids)\n  542  \n  543:         return get_rising(c.site)\n  544  \n  545  class BrowseController(ListingWithPromos):\n  ...\n  637  \n  638  \n  639: class RandomrisingController(ListingWithPromos):\n  640:     where = 'randomrising'\n  641      title_text = _('you\\'re really bored now, eh?')\n  642      next_suggestions_cls = ListingSuggestions\n  643  \n  644      def query(self):\n  645:         links = get_rising(c.site)\n  646  \n  647          if not links:\n  648:             # just pull from the new page if the rising page isn't\n  649              # populated for some reason\n  650              links = c.site.get_links('new', 'all')\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\post.py:\n  158          personalized=VBoolean('pers', default=False),\n  159          discovery=VBoolean('disc', default=False),\n  160:         rising=VBoolean('ris', default=False),\n  161          nsfw=VBoolean('nsfw', default=False),\n  162      )\n  ...\n  166                                personalized,\n  167                                discovery,\n  168:                               rising,\n  169                                nsfw):\n  170          ExploreSettings.record_settings(\n  ...\n  172              personalized=personalized,\n  173              discovery=discovery,\n  174:             rising=rising,\n  175              nsfw=nsfw,\n  176          )\n\nD:\\Git\\reddit\\r2\\r2\\lib\\count.py:\n   26  from pylons import g, config\n   27  \n   28: count_period = g.rising_period\n   29  \n   30  #stubs\n\nD:\\Git\\reddit\\r2\\r2\\lib\\menus.py:\n   60                       saved        = _('saved {toolbar}'),\n   61                       recommended  = _('recommended'),\n   62:                      rising       = _('rising'), \n   63                       admin        = _('admin'), \n   64                                   \n\nD:\\Git\\reddit\\r2\\r2\\lib\\recommender.py:\n   28  from operator import itemgetter\n   29  \n   30: from r2.lib import rising\n   31  from r2.lib.db import operators, tdb_cassandra\n   32  from r2.lib.pages import ExploreItem\n   ..\n   51  \n   52  # explore item types\n   53: TYPE_RISING = _(\"rising\")\n   54  TYPE_DISCOVERY = _(\"discovery\")\n   55  TYPE_HOT = _(\"hot\")\n   ..\n  133      num_recs = 20  # how many recommended srs to ask for\n  134      num_discovery = 2  # how many discovery-related subreddits to mix in\n  135:     num_rising = 4  # how many rising links to mix in\n  136      num_items = 20  # total items to return\n  137:     rising_items = discovery_items = comment_items = hot_items = []\n  138  \n  139      # make a list of srs that shouldn't be recommended\n  ...\n  172          # get links from subreddits dedicated to discovery\n  173          discovery_items = get_hot_items(discovery_srs, TYPE_DISCOVERY, 'disc')\n  174:     if settings.rising:\n  175:         # grab some (non-personalized) rising items\n  176          omit_sr_ids = set(int(id36, 36) for id36 in omit_srid36s)\n  177:         rising_items = get_rising_items(omit_sr_ids, count=num_rising)\n  178      # combine all items and randomize order to get a mix of types\n  179:     all_recs = list(chain(rising_items,\n  180                            comment_items,\n  181                            discovery_items,\n  ...\n  188          if not settings.nsfw and r.is_over18():\n  189              continue\n  190:         if not is_visible(r.sr):  # could happen in rising items\n  191              continue\n  192          if r.sr._id not in seen_srs:\n  ...\n  209  \n  210  \n  211: def get_rising_items(omit_sr_ids, count=4):\n  212:     \"\"\"Get links that are rising right now.\"\"\"\n  213:     all_rising = rising.get_all_rising()\n  214:     candidate_sr_ids = {sr_id for link, score, sr_id in all_rising}.difference(omit_sr_ids)\n  215:     link_fullnames = [link for link, score, sr_id in all_rising if sr_id in candidate_sr_ids]\n  216      link_fullnames_to_show = random_sample(link_fullnames, count)\n  217:     rising_links = Link._by_fullname(link_fullnames_to_show,\n  218                                       return_dict=False,\n  219                                       data=True)\n  220:     rising_items = [ExploreItem(TYPE_RISING, 'ris', Subreddit._byID(l.sr_id), l)\n  221:                    for l in rising_links]\n  222:     return rising_items\n  223  \n  224  \n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   31  \n   32  \n   33: CACHE_KEY = \"rising\"\n   34  \n   35  \n   36: def calc_rising():\n   37      link_counts = count.get_link_counts()\n   38  \n   ..\n   43          return float(link._ups) / max(count, 1)\n   44  \n   45:     # build the rising list, excluding items having 1 or less upvotes\n   46:     rising = []\n   47      for link in links.values():\n   48          if link._ups > 1:\n   49:             rising.append((link._fullname, score(link), link.sr_id))\n   50  \n   51:     # return rising sorted by score\n   52:     return sorted(rising, key=lambda x: x[1], reverse=True)\n   53  \n   54  \n   55: def set_rising():\n   56:     g.cache.set(CACHE_KEY, calc_rising())\n   57  \n   58  \n   59: def get_all_rising():\n   60      return g.cache.get(CACHE_KEY, [])\n   61  \n   62  \n   63: def get_rising(sr):\n   64:     rising = get_all_rising()\n   65:     return [link for link, score, sr_id in rising if sr.keep_for_rising(sr_id)]\n   66  \n   67  \n   68: def get_rising_tuples(sr_ids):\n   69:     rising = get_all_rising()\n   70  \n   71      tuples_by_srid = {sr_id: [] for sr_id in sr_ids}\n   72:     top_rising = {}\n   73  \n   74:     for link, score, sr_id in rising:\n   75          if sr_id not in sr_ids:\n   76              continue\n   77  \n   78:         if sr_id not in top_rising:\n   79:             top_rising[sr_id] = score\n   80  \n   81:         norm_score = score / top_rising[sr_id]\n   82          tuples_by_srid[sr_id].append((-norm_score, -score, link))\n   83  \n   ..\n   85  \n   86  \n   87: def normalized_rising(sr_ids):\n   88      if not sr_ids:\n   89          return []\n   90  \n   91:     tuples_by_srid = sgm(g.cache, sr_ids, miss_fn=get_rising_tuples,\n   92:                          prefix='normalized_rising', time=g.page_cache_time)\n   93  \n   94      merged = heapq.merge(*tuples_by_srid.values())\n\nD:\\Git\\reddit\\r2\\r2\\lib\\wrapped.pyx:\n  499          else:\n  500              self.render_class = self.__class__\n  501:         # this shouldn't be too surprising\n  502          self.cache_ignore = self.cache_ignore.union(\n  503              set(['cachable', 'render', 'cache_ignore', 'lookups']))\n\nD:\\Git\\reddit\\r2\\r2\\lib\\pages\\pages.py:\n  801              main_buttons = [NamedButton('hot', dest='', aliases=['/hot']),\n  802                              NamedButton('new'),\n  803:                             NamedButton('rising'),\n  804                              NamedButton('controversial'),\n  805                              NamedButton('top'),\n\nD:\\Git\\reddit\\r2\\r2\\models\\recommend.py:\n  133      \"\"\"Column family for storing users' view prefs for the /explore page.\"\"\"\n  134      _use_db = True\n  135:     _bool_props = ('personalized', 'discovery', 'rising', 'nsfw')\n  136  \n  137      @classmethod\n  ...\n  148                          personalized=False,\n  149                          discovery=False,\n  150:                         rising=False,\n  151                          nsfw=False):\n  152          \"\"\"Update or create settings for user.\"\"\"\n  ...\n  158                  personalized=personalized,\n  159                  discovery=discovery,\n  160:                 rising=rising,\n  161                  nsfw=nsfw,\n  162              )\n  ...\n  164              settings.personalized = personalized\n  165              settings.discovery = discovery\n  166:             settings.rising = rising\n  167              settings.nsfw = nsfw\n  168          settings._commit()\n  ...\n  174          self.personalized = True\n  175          self.discovery = True\n  176:         self.rising = True\n  177          self.nsfw = False\n  178  \n\nD:\\Git\\reddit\\r2\\r2\\models\\subreddit.py:\n  820          return subreddits if return_dict else subreddits.values()\n  821  \n  822:     def keep_for_rising(self, sr_id):\n  823:         \"\"\"Return whether or not to keep a thing in rising for this SR.\"\"\"\n  824          return sr_id == self._id\n  825  \n  ...\n 1143          BaseSite.__init__(self)\n 1144  \n 1145:     def keep_for_rising(self, sr_id):\n 1146          return False\n 1147  \n ....\n 1266      path = '/r/all'\n 1267  \n 1268:     def keep_for_rising(self, sr_id):\n 1269          return True\n 1270  \n ....\n 1302          self.exclude_sr_ids = [sr._id for sr in srs]\n 1303  \n 1304:     def keep_for_rising(self, sr_id):\n 1305          return sr_id not in self.exclude_sr_ids\n 1306  \n ....\n 1372          return c.defaultsr_cached_sr_ids\n 1373  \n 1374:     def keep_for_rising(self, sr_id):\n 1375          return sr_id in self._get_sr_ids()\n 1376  \n ....\n 1521          return all(sr.allows_referrers for sr in self.srs)\n 1522  \n 1523:     def keep_for_rising(self, sr_id):\n 1524          return sr_id in self.kept_sr_ids\n 1525  \n\nD:\\Git\\reddit\\r2\\r2\\public\\static\\css\\reddit.less:\n 1224  }\n 1225  \n 1226: .explore-rising .explore-label {\n 1227      background-color: #d6fbcb;\n 1228      border: solid thin #485;\n\nD:\\Git\\reddit\\r2\\r2\\public\\static\\js\\lib\\react-with-addons-0.11.2.js:\n 16454      // The actual meaning of the value depends on the users' keyboard layout\n 16455      // which cannot be detected. Assuming that it is a US keyboard layout\n 16456:     // provides a surprisingly accurate mapping for US and European users.\n 16457      // Due to this, it is left to the user to implement at this time.\n 16458      if (event.type === 'keydown' || event.type === 'keyup') {\n\nD:\\Git\\reddit\\r2\\r2\\templates\\exploreitemlisting.html:\n   42              ${_(\"discovery\")}\n   43            </input>\n   44:           <input type=\"checkbox\" name=\"ris\" value=1 ${\"checked\" if thing.settings.rising else \"\"}>\n   45:             ${_(\"rising\")}\n   46            </input>\n   47            <input type=\"checkbox\" name=\"nsfw\" value=1 ${\"checked\" if thing.settings.nsfw else \"\"}>\n\nD:\\Git\\reddit\\upstart\\reddit-job-rising.conf:\n    1: description \"update the rising pages\"\n    2  \n    3  task\n    .\n    9  script\n   10      . /etc/default/reddit\n   11:     wrap-job paster run $REDDIT_INI -c 'from r2.lib import rising; rising.set_rising()'\n   12  end script\n   13  \n\nD:\\Git\\Skimur\\LICENSE:\n  157  12.	RESPONSIBILITY FOR CLAIMS.\n  158  \n  159: As between Initial Developer, Original Developer and the Contributors, each party is responsible for claims and damages arising, directly or indirectly, out of its utilization of rights under this License and You agree to work with Initial Developer, Original Developer and Contributors to distribute such responsibility on an equitable basis. Nothing herein is intended or shall be deemed to constitute any admission of liability.\n  160  \n  161  13.	MULTIPLE-LICENSED CODE.\n\nD:\\Git\\Skimur\\puppet\\modules\\postgresql\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\puppet\\modules\\rabbitmq\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\puppet\\modules\\redis\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\packages\\NUnit.2.6.4\\license.txt:\n    3  Copyright © 2000-2002 Philip A. Craig\n    4  \n    5: This software is provided 'as-is', without any express or implied warranty. In no event will the authors be held liable for any damages arising from the use of this software.\n    6  \n    7  Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions:\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-datepicker\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-datetimepicker\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-modal\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-switch\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\ckeditor\\LICENSE.md:\n 1214  \n 1215       As between Initial Developer and the Contributors, each party is\n 1216:      responsible for claims and damages arising, directly or indirectly,\n 1217       out of its utilization of rights under this License and You agree to\n 1218       work with Initial Developer and Contributors to distribute such\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\examples\\advanced_init\\dt_events.html:\n   49  \n   50  				<p>All custom events fired by DataTables are fired with the namespace <code>dt</code> in order to\n   51: 				prevent conflicts arising with other jQuery plug-ins which also fire events. The DataTables <a href=\n   52  				\"//datatables.net/reference/api/on()\"><code class=\"api\" title=\n   53  				\"DataTables API method\">on()<span>DT</span></code></a> method can be used like the jQuery\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\examples\\advanced_init\\footer_callback.html:\n   74  				\"//datatables.net/reference/option/footerCallback\"><code class=\"option\" title=\n   75  				\"DataTables initialisation option\">footerCallback<span>DT</span></code></a>), it is possible to perform\n   76: 				some powerful and useful data manipulation functions, such as summarising data in the table.</p>\n   77  \n   78  				<p>The example below shows a footer callback being used to total the data for a column (both the\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\media\\js\\jquery.dataTables.js:\n 6640  		 * @param {*} value1 Arrays and/or values to concatenate.\n 6641  		 * @param {*} [...] Additional arrays and/or values to concatenate.\n 6642: 		 * @returns {DataTables.Api} New API instance, comprising of the combined\n 6643  		 *   array.\n 6644  		 */\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\fullcalendar\\fullcalendar.js:\n 4784  \n 4785  	rowEls: null, // set of fake row elements\n 4786: 	dayEls: null, // set of whole-day elements comprising the row's background\n 4787  	helperEls: null, // set of cell skeleton elements for rendering the mock event \"helper\"\n 4788  	highlightEls: null, // set of cell skeleton elements for rendering the highlight\n\n127 matches across 42 files\n\n\nSearching 5266 files for \"calc_rising\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   34  \n   35  \n   36: def calc_rising():\n   37      link_counts = count.get_link_counts()\n   38  \n   ..\n   54  \n   55  def set_rising():\n   56:     g.cache.set(CACHE_KEY, calc_rising())\n   57  \n   58  \n\n2 matches in 1 file\n\n\nSearching 5266 files for \"calc_rising\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   34  \n   35  \n   36: def calc_rising():\n   37      link_counts = count.get_link_counts()\n   38  \n   ..\n   54  \n   55  def set_rising():\n   56:     g.cache.set(CACHE_KEY, calc_rising())\n   57  \n   58  \n\n2 matches in 1 file\n\n\nSearching 5266 files for \"set_rising\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   53  \n   54  \n   55: def set_rising():\n   56      g.cache.set(CACHE_KEY, calc_rising())\n   57  \n\nD:\\Git\\reddit\\upstart\\reddit-job-rising.conf:\n    9  script\n   10      . /etc/default/reddit\n   11:     wrap-job paster run $REDDIT_INI -c 'from r2.lib import rising; rising.set_rising()'\n   12  end script\n   13  \n\n2 matches across 2 files\n\n\nSearching 5266 files for \"calc_rising\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   34  \n   35  \n   36: def calc_rising():\n   37      link_counts = count.get_link_counts()\n   38  \n   ..\n   54  \n   55  def set_rising():\n   56:     g.cache.set(CACHE_KEY, calc_rising())\n   57  \n   58  \n\n2 matches in 1 file\n\n\nSearching 5266 files for \"rising\" (case sensitive)\n\nD:\\Git\\reddit\\install-reddit.sh:\n  763  */5  * * * * root /sbin/start --quiet reddit-job-clean_up_hardcache\n  764  */2  * * * * root /sbin/start --quiet reddit-job-broken_things\n  765: */2  * * * * root /sbin/start --quiet reddit-job-rising\n  766  0    * * * * root /sbin/start --quiet reddit-job-trylater\n  767  \n\nD:\\Git\\reddit\\LICENSE:\n  395  \n  396  As between Initial Developer, Original Developer and the Contributors, each\n  397: party is responsible for claims and damages arising, directly or indirectly, out\n  398  of its utilization of rights under this License and You agree to work with\n  399  Initial Developer, Original Developer and Contributors to distribute such\n\nD:\\Git\\reddit\\r2\\example.ini:\n  398  # maximum age (in days) of items eligible for display on normalized hot pages (frontpage, multis, etc.)\n  399  HOT_PAGE_AGE = 1000\n  400: # how long to consider links eligible for the rising page\n  401: rising_period = 12 hours\n  402  # default number of comments shown\n  403  num_comments = 100\n\nD:\\Git\\reddit\\r2\\r2\\config\\routing.py:\n  160            action='listing', requirements=dict(sort='top|controversial'))\n  161         connect('/:controller', action='listing',\n  162:           requirements=dict(controller=\"hot|new|rising|randomrising|ads\"))\n  163  \n  164      mc('/user/:username/:where/:show', controller='user', action='listing')\n  ...\n  274  \n  275      mc('/:controller', action='listing',\n  276:        requirements=dict(controller=\"hot|new|rising|randomrising|ads\"))\n  277      mc('/saved', controller='user', action='saved_redirect')\n  278  \n\nD:\\Git\\reddit\\r2\\r2\\controllers\\__init__.py:\n   50      from listingcontroller import RedditsController\n   51      from listingcontroller import ByIDController\n   52:     from listingcontroller import RandomrisingController\n   53      from listingcontroller import UserController\n   54      from listingcontroller import CommentsController\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\listingcontroller.py:\n   36  from r2.lib.menus import TimeMenu, SortMenu, RecSortMenu, ProfileSortMenu\n   37  from r2.lib.menus import ControversyTimeMenu, menu, QueryButton\n   38: from r2.lib.rising import get_rising, normalized_rising\n   39  from r2.lib.wrapped import Wrapped\n   40  from r2.lib.normalized_hot import normalized_hot\n   ..\n  524      @listing_api_doc(uri='/new', uses_site=True)\n  525      def GET_listing(self, **env):\n  526:         if request.params.get('sort') == 'rising':\n  527:             return self.redirect(add_sr('/rising'))\n  528  \n  529          return ListingController.GET_listing(self, **env)\n  530  \n  531  class RisingController(NewController):\n  532:     where = 'rising'\n  533:     title_text = _('rising submissions')\n  534:     extra_page_classes = ListingController.extra_page_classes + ['rising-page']\n  535  \n  536      def query(self):\n  537          if isinstance(c.site, DefaultSR):\n  538              sr_ids = Subreddit.user_subreddits(c.user)\n  539:             return normalized_rising(sr_ids)\n  540          elif isinstance(c.site, MultiReddit):\n  541:             return normalized_rising(c.site.kept_sr_ids)\n  542  \n  543:         return get_rising(c.site)\n  544  \n  545  class BrowseController(ListingWithPromos):\n  ...\n  637  \n  638  \n  639: class RandomrisingController(ListingWithPromos):\n  640:     where = 'randomrising'\n  641      title_text = _('you\\'re really bored now, eh?')\n  642      next_suggestions_cls = ListingSuggestions\n  643  \n  644      def query(self):\n  645:         links = get_rising(c.site)\n  646  \n  647          if not links:\n  648:             # just pull from the new page if the rising page isn't\n  649              # populated for some reason\n  650              links = c.site.get_links('new', 'all')\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\post.py:\n  158          personalized=VBoolean('pers', default=False),\n  159          discovery=VBoolean('disc', default=False),\n  160:         rising=VBoolean('ris', default=False),\n  161          nsfw=VBoolean('nsfw', default=False),\n  162      )\n  ...\n  166                                personalized,\n  167                                discovery,\n  168:                               rising,\n  169                                nsfw):\n  170          ExploreSettings.record_settings(\n  ...\n  172              personalized=personalized,\n  173              discovery=discovery,\n  174:             rising=rising,\n  175              nsfw=nsfw,\n  176          )\n\nD:\\Git\\reddit\\r2\\r2\\lib\\count.py:\n   26  from pylons import g, config\n   27  \n   28: count_period = g.rising_period\n   29  \n   30  #stubs\n\nD:\\Git\\reddit\\r2\\r2\\lib\\menus.py:\n   60                       saved        = _('saved {toolbar}'),\n   61                       recommended  = _('recommended'),\n   62:                      rising       = _('rising'), \n   63                       admin        = _('admin'), \n   64                                   \n\nD:\\Git\\reddit\\r2\\r2\\lib\\recommender.py:\n   28  from operator import itemgetter\n   29  \n   30: from r2.lib import rising\n   31  from r2.lib.db import operators, tdb_cassandra\n   32  from r2.lib.pages import ExploreItem\n   ..\n   51  \n   52  # explore item types\n   53: TYPE_RISING = _(\"rising\")\n   54  TYPE_DISCOVERY = _(\"discovery\")\n   55  TYPE_HOT = _(\"hot\")\n   ..\n  133      num_recs = 20  # how many recommended srs to ask for\n  134      num_discovery = 2  # how many discovery-related subreddits to mix in\n  135:     num_rising = 4  # how many rising links to mix in\n  136      num_items = 20  # total items to return\n  137:     rising_items = discovery_items = comment_items = hot_items = []\n  138  \n  139      # make a list of srs that shouldn't be recommended\n  ...\n  172          # get links from subreddits dedicated to discovery\n  173          discovery_items = get_hot_items(discovery_srs, TYPE_DISCOVERY, 'disc')\n  174:     if settings.rising:\n  175:         # grab some (non-personalized) rising items\n  176          omit_sr_ids = set(int(id36, 36) for id36 in omit_srid36s)\n  177:         rising_items = get_rising_items(omit_sr_ids, count=num_rising)\n  178      # combine all items and randomize order to get a mix of types\n  179:     all_recs = list(chain(rising_items,\n  180                            comment_items,\n  181                            discovery_items,\n  ...\n  188          if not settings.nsfw and r.is_over18():\n  189              continue\n  190:         if not is_visible(r.sr):  # could happen in rising items\n  191              continue\n  192          if r.sr._id not in seen_srs:\n  ...\n  209  \n  210  \n  211: def get_rising_items(omit_sr_ids, count=4):\n  212:     \"\"\"Get links that are rising right now.\"\"\"\n  213:     all_rising = rising.get_all_rising()\n  214:     candidate_sr_ids = {sr_id for link, score, sr_id in all_rising}.difference(omit_sr_ids)\n  215:     link_fullnames = [link for link, score, sr_id in all_rising if sr_id in candidate_sr_ids]\n  216      link_fullnames_to_show = random_sample(link_fullnames, count)\n  217:     rising_links = Link._by_fullname(link_fullnames_to_show,\n  218                                       return_dict=False,\n  219                                       data=True)\n  220:     rising_items = [ExploreItem(TYPE_RISING, 'ris', Subreddit._byID(l.sr_id), l)\n  221:                    for l in rising_links]\n  222:     return rising_items\n  223  \n  224  \n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   31  \n   32  \n   33: CACHE_KEY = \"rising\"\n   34  \n   35  \n   36: def calc_rising():\n   37      link_counts = count.get_link_counts()\n   38  \n   ..\n   43          return float(link._ups) / max(count, 1)\n   44  \n   45:     # build the rising list, excluding items having 1 or less upvotes\n   46:     rising = []\n   47      for link in links.values():\n   48          if link._ups > 1:\n   49:             rising.append((link._fullname, score(link), link.sr_id))\n   50  \n   51:     # return rising sorted by score\n   52:     return sorted(rising, key=lambda x: x[1], reverse=True)\n   53  \n   54  \n   55: def set_rising():\n   56:     g.cache.set(CACHE_KEY, calc_rising())\n   57  \n   58  \n   59: def get_all_rising():\n   60      return g.cache.get(CACHE_KEY, [])\n   61  \n   62  \n   63: def get_rising(sr):\n   64:     rising = get_all_rising()\n   65:     return [link for link, score, sr_id in rising if sr.keep_for_rising(sr_id)]\n   66  \n   67  \n   68: def get_rising_tuples(sr_ids):\n   69:     rising = get_all_rising()\n   70  \n   71      tuples_by_srid = {sr_id: [] for sr_id in sr_ids}\n   72:     top_rising = {}\n   73  \n   74:     for link, score, sr_id in rising:\n   75          if sr_id not in sr_ids:\n   76              continue\n   77  \n   78:         if sr_id not in top_rising:\n   79:             top_rising[sr_id] = score\n   80  \n   81:         norm_score = score / top_rising[sr_id]\n   82          tuples_by_srid[sr_id].append((-norm_score, -score, link))\n   83  \n   ..\n   85  \n   86  \n   87: def normalized_rising(sr_ids):\n   88      if not sr_ids:\n   89          return []\n   90  \n   91:     tuples_by_srid = sgm(g.cache, sr_ids, miss_fn=get_rising_tuples,\n   92:                          prefix='normalized_rising', time=g.page_cache_time)\n   93  \n   94      merged = heapq.merge(*tuples_by_srid.values())\n\nD:\\Git\\reddit\\r2\\r2\\lib\\wrapped.pyx:\n  499          else:\n  500              self.render_class = self.__class__\n  501:         # this shouldn't be too surprising\n  502          self.cache_ignore = self.cache_ignore.union(\n  503              set(['cachable', 'render', 'cache_ignore', 'lookups']))\n\nD:\\Git\\reddit\\r2\\r2\\lib\\pages\\pages.py:\n  801              main_buttons = [NamedButton('hot', dest='', aliases=['/hot']),\n  802                              NamedButton('new'),\n  803:                             NamedButton('rising'),\n  804                              NamedButton('controversial'),\n  805                              NamedButton('top'),\n\nD:\\Git\\reddit\\r2\\r2\\models\\recommend.py:\n  133      \"\"\"Column family for storing users' view prefs for the /explore page.\"\"\"\n  134      _use_db = True\n  135:     _bool_props = ('personalized', 'discovery', 'rising', 'nsfw')\n  136  \n  137      @classmethod\n  ...\n  148                          personalized=False,\n  149                          discovery=False,\n  150:                         rising=False,\n  151                          nsfw=False):\n  152          \"\"\"Update or create settings for user.\"\"\"\n  ...\n  158                  personalized=personalized,\n  159                  discovery=discovery,\n  160:                 rising=rising,\n  161                  nsfw=nsfw,\n  162              )\n  ...\n  164              settings.personalized = personalized\n  165              settings.discovery = discovery\n  166:             settings.rising = rising\n  167              settings.nsfw = nsfw\n  168          settings._commit()\n  ...\n  174          self.personalized = True\n  175          self.discovery = True\n  176:         self.rising = True\n  177          self.nsfw = False\n  178  \n\nD:\\Git\\reddit\\r2\\r2\\models\\subreddit.py:\n  820          return subreddits if return_dict else subreddits.values()\n  821  \n  822:     def keep_for_rising(self, sr_id):\n  823:         \"\"\"Return whether or not to keep a thing in rising for this SR.\"\"\"\n  824          return sr_id == self._id\n  825  \n  ...\n 1143          BaseSite.__init__(self)\n 1144  \n 1145:     def keep_for_rising(self, sr_id):\n 1146          return False\n 1147  \n ....\n 1266      path = '/r/all'\n 1267  \n 1268:     def keep_for_rising(self, sr_id):\n 1269          return True\n 1270  \n ....\n 1302          self.exclude_sr_ids = [sr._id for sr in srs]\n 1303  \n 1304:     def keep_for_rising(self, sr_id):\n 1305          return sr_id not in self.exclude_sr_ids\n 1306  \n ....\n 1372          return c.defaultsr_cached_sr_ids\n 1373  \n 1374:     def keep_for_rising(self, sr_id):\n 1375          return sr_id in self._get_sr_ids()\n 1376  \n ....\n 1521          return all(sr.allows_referrers for sr in self.srs)\n 1522  \n 1523:     def keep_for_rising(self, sr_id):\n 1524          return sr_id in self.kept_sr_ids\n 1525  \n\nD:\\Git\\reddit\\r2\\r2\\public\\static\\css\\reddit.less:\n 1224  }\n 1225  \n 1226: .explore-rising .explore-label {\n 1227      background-color: #d6fbcb;\n 1228      border: solid thin #485;\n\nD:\\Git\\reddit\\r2\\r2\\public\\static\\js\\lib\\react-with-addons-0.11.2.js:\n 16454      // The actual meaning of the value depends on the users' keyboard layout\n 16455      // which cannot be detected. Assuming that it is a US keyboard layout\n 16456:     // provides a surprisingly accurate mapping for US and European users.\n 16457      // Due to this, it is left to the user to implement at this time.\n 16458      if (event.type === 'keydown' || event.type === 'keyup') {\n\nD:\\Git\\reddit\\r2\\r2\\templates\\exploreitemlisting.html:\n   42              ${_(\"discovery\")}\n   43            </input>\n   44:           <input type=\"checkbox\" name=\"ris\" value=1 ${\"checked\" if thing.settings.rising else \"\"}>\n   45:             ${_(\"rising\")}\n   46            </input>\n   47            <input type=\"checkbox\" name=\"nsfw\" value=1 ${\"checked\" if thing.settings.nsfw else \"\"}>\n\nD:\\Git\\reddit\\upstart\\reddit-job-rising.conf:\n    1: description \"update the rising pages\"\n    2  \n    3  task\n    .\n    9  script\n   10      . /etc/default/reddit\n   11:     wrap-job paster run $REDDIT_INI -c 'from r2.lib import rising; rising.set_rising()'\n   12  end script\n   13  \n\nD:\\Git\\Skimur\\LICENSE:\n  157  12.	RESPONSIBILITY FOR CLAIMS.\n  158  \n  159: As between Initial Developer, Original Developer and the Contributors, each party is responsible for claims and damages arising, directly or indirectly, out of its utilization of rights under this License and You agree to work with Initial Developer, Original Developer and Contributors to distribute such responsibility on an equitable basis. Nothing herein is intended or shall be deemed to constitute any admission of liability.\n  160  \n  161  13.	MULTIPLE-LICENSED CODE.\n\nD:\\Git\\Skimur\\puppet\\modules\\postgresql\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\puppet\\modules\\rabbitmq\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\puppet\\modules\\redis\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\packages\\NUnit.2.6.4\\license.txt:\n    3  Copyright © 2000-2002 Philip A. Craig\n    4  \n    5: This software is provided 'as-is', without any express or implied warranty. In no event will the authors be held liable for any damages arising from the use of this software.\n    6  \n    7  Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions:\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amcharts\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\ammap\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\dataloader\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\export\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\amcharts\\amstockcharts\\plugins\\responsive\\license.txt:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-datepicker\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-datetimepicker\\LICENSE:\n  157        negligent acts) or agreed to in writing, shall any Contributor be\n  158        liable to You for damages, including any direct, indirect, special,\n  159:       incidental, or consequential damages of any character arising as a\n  160        result of this License or out of the use or inability to use the\n  161        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-modal\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\bootstrap-switch\\LICENSE:\n  156        negligent acts) or agreed to in writing, shall any Contributor be\n  157        liable to You for damages, including any direct, indirect, special,\n  158:       incidental, or consequential damages of any character arising as a\n  159        result of this License or out of the use or inability to use the\n  160        Work (including but not limited to damages for loss of goodwill,\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\ckeditor\\LICENSE.md:\n 1214  \n 1215       As between Initial Developer and the Contributors, each party is\n 1216:      responsible for claims and damages arising, directly or indirectly,\n 1217       out of its utilization of rights under this License and You agree to\n 1218       work with Initial Developer and Contributors to distribute such\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\examples\\advanced_init\\dt_events.html:\n   49  \n   50  				<p>All custom events fired by DataTables are fired with the namespace <code>dt</code> in order to\n   51: 				prevent conflicts arising with other jQuery plug-ins which also fire events. The DataTables <a href=\n   52  				\"//datatables.net/reference/api/on()\"><code class=\"api\" title=\n   53  				\"DataTables API method\">on()<span>DT</span></code></a> method can be used like the jQuery\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\examples\\advanced_init\\footer_callback.html:\n   74  				\"//datatables.net/reference/option/footerCallback\"><code class=\"option\" title=\n   75  				\"DataTables initialisation option\">footerCallback<span>DT</span></code></a>), it is possible to perform\n   76: 				some powerful and useful data manipulation functions, such as summarising data in the table.</p>\n   77  \n   78  				<p>The example below shows a footer callback being used to total the data for a column (both the\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\datatables\\media\\js\\jquery.dataTables.js:\n 6640  		 * @param {*} value1 Arrays and/or values to concatenate.\n 6641  		 * @param {*} [...] Additional arrays and/or values to concatenate.\n 6642: 		 * @returns {DataTables.Api} New API instance, comprising of the combined\n 6643  		 *   array.\n 6644  		 */\n\nD:\\Git\\Skimur\\src\\temptheme\\v4.0.2\\theme\\assets\\global\\plugins\\fullcalendar\\fullcalendar.js:\n 4784  \n 4785  	rowEls: null, // set of fake row elements\n 4786: 	dayEls: null, // set of whole-day elements comprising the row's background\n 4787  	helperEls: null, // set of cell skeleton elements for rendering the mock event \"helper\"\n 4788  	highlightEls: null, // set of cell skeleton elements for rendering the highlight\n\n127 matches across 42 files\n\n\nSearching 5266 files for \"normalized_rising\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\listingcontroller.py:\n   36  from r2.lib.menus import TimeMenu, SortMenu, RecSortMenu, ProfileSortMenu\n   37  from r2.lib.menus import ControversyTimeMenu, menu, QueryButton\n   38: from r2.lib.rising import get_rising, normalized_rising\n   39  from r2.lib.wrapped import Wrapped\n   40  from r2.lib.normalized_hot import normalized_hot\n   ..\n  537          if isinstance(c.site, DefaultSR):\n  538              sr_ids = Subreddit.user_subreddits(c.user)\n  539:             return normalized_rising(sr_ids)\n  540          elif isinstance(c.site, MultiReddit):\n  541:             return normalized_rising(c.site.kept_sr_ids)\n  542  \n  543          return get_rising(c.site)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   85  \n   86  \n   87: def normalized_rising(sr_ids):\n   88      if not sr_ids:\n   89          return []\n   90  \n   91      tuples_by_srid = sgm(g.cache, sr_ids, miss_fn=get_rising_tuples,\n   92:                          prefix='normalized_rising', time=g.page_cache_time)\n   93  \n   94      merged = heapq.merge(*tuples_by_srid.values())\n\n5 matches across 2 files\n\n\nSearching 5266 files for \"normalized_rising\" (case sensitive)\n\nD:\\Git\\reddit\\r2\\r2\\controllers\\listingcontroller.py:\n   36  from r2.lib.menus import TimeMenu, SortMenu, RecSortMenu, ProfileSortMenu\n   37  from r2.lib.menus import ControversyTimeMenu, menu, QueryButton\n   38: from r2.lib.rising import get_rising, normalized_rising\n   39  from r2.lib.wrapped import Wrapped\n   40  from r2.lib.normalized_hot import normalized_hot\n   ..\n  537          if isinstance(c.site, DefaultSR):\n  538              sr_ids = Subreddit.user_subreddits(c.user)\n  539:             return normalized_rising(sr_ids)\n  540          elif isinstance(c.site, MultiReddit):\n  541:             return normalized_rising(c.site.kept_sr_ids)\n  542  \n  543          return get_rising(c.site)\n\nD:\\Git\\reddit\\r2\\r2\\lib\\rising.py:\n   85  \n   86  \n   87: def normalized_rising(sr_ids):\n   88      if not sr_ids:\n   89          return []\n   90  \n   91      tuples_by_srid = sgm(g.cache, sr_ids, miss_fn=get_rising_tuples,\n   92:                          prefix='normalized_rising', time=g.page_cache_time)\n   93  \n   94      merged = heapq.merge(*tuples_by_srid.values())\n\n5 matches across 2 files\n",
			"settings":
			{
				"buffer_size": 79399,
				"line_ending": "Windows",
				"name": "Find Results",
				"scratch": true
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/rising.py",
			"settings":
			{
				"buffer_size": 2889,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/count.py",
			"settings":
			{
				"buffer_size": 1793,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/db/tdb_sql.py",
			"settings":
			{
				"buffer_size": 33941,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/db/tdb_lite.py",
			"settings":
			{
				"buffer_size": 2927,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/controllers/listingcontroller.py",
			"settings":
			{
				"buffer_size": 66022,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/app_globals.py",
			"settings":
			{
				"buffer_size": 34447,
				"line_ending": "Windows"
			}
		},
		{
			"file": "src/packages/repositories.config",
			"settings":
			{
				"buffer_size": 873,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/eventcollector.py",
			"settings":
			{
				"buffer_size": 11465,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/models/listing.py",
			"settings":
			{
				"buffer_size": 11808,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/example.ini",
			"settings":
			{
				"buffer_size": 29821,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/organic.py",
			"settings":
			{
				"buffer_size": 2739,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/models/link.py",
			"settings":
			{
				"buffer_size": 93535,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/models/query_cache.py",
			"settings":
			{
				"buffer_size": 21630,
				"line_ending": "Windows"
			}
		},
		{
			"contents": "# The contents of this file are subject to the Common Public Attribution\n# License Version 1.0. (the \"License\"); you may not use this file except in\n# compliance with the License. You may obtain a copy of the License at\n# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public\n# License Version 1.1, but Sections 14 and 15 have been added to cover use of\n# software over a computer network and provide for limited attribution for the\n# Original Developer. In addition, Exhibit A has been modified to be consistent\n# with Exhibit B.\n#\n# Software distributed under the License is distributed on an \"AS IS\" basis,\n# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for\n# the specific language governing rights and limitations under the License.\n#\n# The Original Code is reddit.\n#\n# The Original Developer is the Initial Developer.  The Initial Developer of\n# the Original Code is reddit Inc.\n#\n# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit\n# Inc. All Rights Reserved.\n###############################################################################\nimport json\n\nfrom r2.models import Account, Link, Comment, Report, LinksByAccount\nfrom r2.models.vote import cast_vote, get_votes, VotesByAccount\nfrom r2.models import Message, Inbox, Subreddit, ModContribSR, ModeratorInbox, MultiReddit\nfrom r2.lib.db.thing import Thing, Merge\nfrom r2.lib.db.operators import asc, desc, timeago\nfrom r2.lib.db.sorts import epoch_seconds\nfrom r2.lib.db import tdb_cassandra\nfrom r2.lib.utils import fetch_things2, tup, UniqueIterator, set_last_modified\nfrom r2.lib import utils\nfrom r2.lib import amqp, sup, filters\nfrom r2.lib.comment_tree import add_comments, update_comment_votes\nfrom r2.models.promo import PROMOTE_STATUS, PromotionLog\nfrom r2.models.query_cache import (\n    cached_query,\n    CachedQuery,\n    CachedQueryMutator,\n    filter_thing,\n    FakeQuery,\n    merged_cached_query,\n    MergedCachedQuery,\n    SubredditQueryCache,\n    ThingTupleComparator,\n    UserQueryCache,\n)\nfrom r2.models.last_modified import LastModified\nfrom r2.lib.utils import in_chunks, is_subdomain, SimpleSillyStub\n\nimport cPickle as pickle\n\nfrom datetime import datetime\nfrom time import mktime\nimport pytz\nimport itertools\nimport collections\nfrom copy import deepcopy\nfrom r2.lib.db.operators import and_, or_\n\nfrom pylons import g\nquery_cache = g.permacache\nlog = g.log\nmake_lock = g.make_lock\nworker = amqp.worker\nstats = g.stats\n\nprecompute_limit = 1000\n\ndb_sorts = dict(hot = (desc, '_hot'),\n                new = (desc, '_date'),\n                top = (desc, '_score'),\n                controversial = (desc, '_controversy'))\n\ndef db_sort(sort):\n    cls, col = db_sorts[sort]\n    return cls(col)\n\ndb_times = dict(all = None,\n                hour = Thing.c._date >= timeago('1 hour'),\n                day = Thing.c._date >= timeago('1 day'),\n                week = Thing.c._date >= timeago('1 week'),\n                month = Thing.c._date >= timeago('1 month'),\n                year = Thing.c._date >= timeago('1 year'))\n\n# sorts for which there can be a time filter (by day, by week,\n# etc). All of these but 'all' are done in mr_top, who knows about the\n# structure of the stored CachedResults (so changes here may warrant\n# changes there)\ntime_filtered_sorts = set(('top', 'controversial'))\n\n#we need to define the filter functions here so cachedresults can be pickled\ndef filter_identity(x):\n    return x\n\ndef filter_thing2(x):\n    \"\"\"A filter to apply to the results of a relationship query returns\n    the object of the relationship.\"\"\"\n    return x._thing2\n\nclass CachedResults(object):\n    \"\"\"Given a query returns a list-like object that will lazily look up\n    the query from the persistent cache. \"\"\"\n    def __init__(self, query, filter):\n        self.query = query\n        self.query._limit = precompute_limit\n        self.filter = filter\n        self.iden = self.query._iden()\n        self.sort_cols = [s.col for s in self.query._sort]\n        self.data = []\n        self._fetched = False\n\n    @property\n    def sort(self):\n        return self.query._sort\n\n    def fetch(self, force=False, stale=False):\n        \"\"\"Loads the query from the cache.\"\"\"\n        self.fetch_multi([self], force=force, stale=stale)\n\n    @classmethod\n    def fetch_multi(cls, crs, force=False, stale=False):\n        unfetched = filter(lambda cr: force or not cr._fetched, crs)\n        if not unfetched:\n            return\n\n        keys = [cr.iden for cr in unfetched]\n        cached = query_cache.get_multi(keys, allow_local=not force, stale=stale)\n        for cr in unfetched:\n            cr.data = cached.get(cr.iden) or []\n            cr._fetched = True\n\n    def make_item_tuple(self, item):\n        \"\"\"Given a single 'item' from the result of a query build the tuple\n        that will be stored in the query cache. It is effectively the\n        fullname of the item after passing through the filter plus the\n        columns of the unfiltered item to sort by.\"\"\"\n        filtered_item = self.filter(item)\n        lst = [filtered_item._fullname]\n        for col in self.sort_cols:\n            #take the property of the original \n            attr = getattr(item, col)\n            #convert dates to epochs to take less space\n            if isinstance(attr, datetime):\n                attr = epoch_seconds(attr)\n            lst.append(attr)\n        return tuple(lst)\n\n    def can_insert(self):\n        \"\"\"True if a new item can just be inserted rather than\n           rerunning the query.\"\"\"\n         # This is only true in some circumstances: queries where\n         # eligibility in the list is determined only by its sort\n         # value (e.g. hot) and where addition/removal from the list\n         # incurs an insertion/deletion event called on the query. So\n         # the top hottest items in X some subreddit where the query\n         # is notified on every submission/banning/unbanning/deleting\n         # will work, but for queries with a time-component or some\n         # other eligibility factor, it cannot be inserted this way.\n        if self.query._sort in ([desc('_date')],\n                                [desc('_hot'), desc('_date')],\n                                [desc('_score'), desc('_date')],\n                                [desc('_controversy'), desc('_date')]):\n            if not any(r for r in self.query._rules\n                       if r.lval.name == '_date'):\n                # if no time-rule is specified, then it's 'all'\n                return True\n        return False\n\n    def can_delete(self):\n        \"True if a item can be removed from the listing, always true for now.\"\n        return True\n\n    def _mutate(self, fn, willread=True):\n        self.data = query_cache.mutate(self.iden, fn, default=[], willread=willread)\n        self._fetched=True\n\n    def insert(self, items):\n        \"\"\"Inserts the item into the cached data. This only works\n           under certain criteria, see can_insert.\"\"\"\n        self._insert_tuples([self.make_item_tuple(item) for item in tup(items)])\n\n    def _insert_tuples(self, tuples):\n        def _mutate(data):\n            data = data or []\n            item_tuples = tuples or []\n\n            existing_fnames = {item[0] for item in data}\n            new_fnames = {item[0] for item in item_tuples}\n\n            mutated_length = len(existing_fnames.union(new_fnames))\n            would_truncate = mutated_length >= precompute_limit\n            if would_truncate:\n                # only insert items that are already stored or new items\n                # that are large enough that they won't be immediately truncated\n                # out of storage\n                # item structure is (name, sortval1[, sortval2, ...])\n                smallest = data[-1]\n                item_tuples = [item for item in item_tuples\n                                    if (item[0] in existing_fnames or\n                                        item[1:] >= smallest[1:])]\n\n            if not item_tuples:\n                return data\n\n            # insert the items, remove the duplicates (keeping the\n            # one being inserted over the stored value if applicable),\n            # and sort the result\n            data = filter(lambda x: x[0] not in new_fnames, data)\n            data.extend(item_tuples)\n            data.sort(reverse=True, key=lambda x: x[1:])\n            if len(data) > precompute_limit:\n                data = data[:precompute_limit]\n            return data\n\n        self._mutate(_mutate)\n\n    def delete(self, items):\n        \"\"\"Deletes an item from the cached data.\"\"\"\n        fnames = set(self.filter(x)._fullname for x in tup(items))\n\n        def _mutate(data):\n            data = data or []\n            return filter(lambda x: x[0] not in fnames,\n                          data)\n\n        self._mutate(_mutate)\n\n    def _replace(self, tuples):\n        \"\"\"Take pre-rendered tuples from mr_top and replace the\n           contents of the query outright. This should be considered a\n           private API\"\"\"\n        def _mutate(data):\n            return tuples\n        self._mutate(_mutate, willread=False)\n\n    def update(self):\n        \"\"\"Runs the query and stores the result in the cache. This is\n           only run by hand.\"\"\"\n        self.data = [self.make_item_tuple(i) for i in self.query]\n        self._fetched = True\n        query_cache.set(self.iden, self.data)\n\n    def __repr__(self):\n        return '<CachedResults %s %s>' % (self.query._rules, self.query._sort)\n\n    def __iter__(self):\n        self.fetch()\n\n        for x in self.data:\n            yield x[0]\n\nclass MergedCachedResults(object):\n    \"\"\"Given two CachedResults, merges their lists based on the sorts\n       of their queries.\"\"\"\n    # normally we'd do this by having a superclass of CachedResults,\n    # but we have legacy pickled CachedResults that we don't want to\n    # break\n\n    def __init__(self, results):\n        self.cached_results = results\n        CachedResults.fetch_multi([r for r in results\n                                   if isinstance(r, CachedResults)])\n        CachedQuery._fetch_multi([r for r in results\n                                   if isinstance(r, CachedQuery)])\n        self._fetched = True\n\n        self.sort = results[0].sort\n        comparator = ThingTupleComparator(self.sort)\n        # make sure they're all the same\n        assert all(r.sort == self.sort for r in results[1:])\n\n        all_items = []\n        for cr in results:\n            all_items.extend(cr.data)\n        all_items.sort(cmp=comparator)\n        self.data = all_items\n\n\n    def __repr__(self):\n        return '<MergedCachedResults %r>' % (self.cached_results,)\n\n    def __iter__(self):\n        for x in self.data:\n            yield x[0]\n\n    def update(self):\n        for x in self.cached_results:\n            x.update()\n\ndef make_results(query, filter = filter_identity):\n    return CachedResults(query, filter)\n\ndef merge_results(*results):\n    if not results:\n        return []\n    return MergedCachedResults(results)\n\ndef migrating_cached_query(model, filter_fn=filter_identity):\n    \"\"\"Returns a CachedResults object that has a new-style cached query\n    attached as \"new_query\". This way, reads will happen from the old\n    query cache while writes can be made to go to both caches until a\n    backfill migration is complete.\"\"\"\n\n    decorator = cached_query(model, filter_fn)\n    def migrating_cached_query_decorator(fn):\n        wrapped = decorator(fn)\n        def migrating_cached_query_wrapper(*args):\n            new_query = wrapped(*args)\n            old_query = make_results(new_query.query, filter_fn)\n            old_query.new_query = new_query\n            return old_query\n        return migrating_cached_query_wrapper\n    return migrating_cached_query_decorator\n\n\n@cached_query(UserQueryCache)\ndef get_deleted_links(user_id):\n    return Link._query(Link.c.author_id == user_id,\n                       Link.c._deleted == True,\n                       Link.c._spam == (True, False),\n                       sort=db_sort('new'))\n\n\n@cached_query(UserQueryCache)\ndef get_deleted_comments(user_id):\n    return Comment._query(Comment.c.author_id == user_id,\n                          Comment.c._deleted == True,\n                          Comment.c._spam == (True, False),\n                          sort=db_sort('new'))\n\n\n@merged_cached_query\ndef get_deleted(user):\n    return [get_deleted_links(user),\n            get_deleted_comments(user)]\n\n\ndef get_links(sr, sort, time):\n    return _get_links(sr._id, sort, time)\n\ndef _get_links(sr_id, sort, time):\n    \"\"\"General link query for a subreddit.\"\"\"\n    q = Link._query(Link.c.sr_id == sr_id,\n                    sort = db_sort(sort),\n                    data = True)\n\n    if time != 'all':\n        q._filter(db_times[time])\n\n    res = make_results(q)\n\n    return res\n\n@cached_query(SubredditQueryCache)\ndef get_spam_links(sr_id):\n    return Link._query(Link.c.sr_id == sr_id,\n                       Link.c._spam == True,\n                       sort = db_sort('new'))\n\n@cached_query(SubredditQueryCache)\ndef get_spam_comments(sr_id):\n    return Comment._query(Comment.c.sr_id == sr_id,\n                          Comment.c._spam == True,\n                          sort = db_sort('new'))\n\n\n@cached_query(SubredditQueryCache)\ndef get_edited_comments(sr_id):\n    return FakeQuery(sort=[desc(\"editted\")])\n\n\n@cached_query(SubredditQueryCache)\ndef get_edited_links(sr_id):\n    return FakeQuery(sort=[desc(\"editted\")])\n\n\n@merged_cached_query\ndef get_edited(sr, user=None, include_links=True, include_comments=True):\n    sr_ids = moderated_srids(sr, user)\n    queries = []\n\n    if include_links:\n        queries.append(get_edited_links)\n    if include_comments:\n        queries.append(get_edited_comments)\n    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]\n\n\ndef moderated_srids(sr, user):\n    if isinstance(sr, (ModContribSR, MultiReddit)):\n        srs = Subreddit._byID(sr.sr_ids, return_dict=False)\n        if user:\n            srs = [sr for sr in srs\n                   if sr.is_moderator_with_perms(user, 'posts')]\n        return [sr._id for sr in srs]\n    else:\n        return [sr._id]\n\n@merged_cached_query\ndef get_spam(sr, user=None, include_links=True, include_comments=True):\n    sr_ids = moderated_srids(sr, user)\n    queries = []\n\n    if include_links:\n        queries.append(get_spam_links)\n    if include_comments:\n        queries.append(get_spam_comments)\n    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]\n\n@cached_query(SubredditQueryCache)\ndef get_spam_filtered_links(sr_id):\n    \"\"\" NOTE: This query will never run unless someone does an \"update\" on it,\n        but that will probably timeout. Use insert_spam_filtered_links.\"\"\"\n    return Link._query(Link.c.sr_id == sr_id,\n                       Link.c._spam == True,\n                       Link.c.verdict != 'mod-removed',\n                       sort = db_sort('new'))\n\n@cached_query(SubredditQueryCache)\ndef get_spam_filtered_comments(sr_id):\n    return Comment._query(Comment.c.sr_id == sr_id,\n                          Comment.c._spam == True,\n                          Comment.c.verdict != 'mod-removed',\n                          sort = db_sort('new'))\n\n@merged_cached_query\ndef get_spam_filtered(sr):\n    return [get_spam_filtered_links(sr),\n            get_spam_filtered_comments(sr)]\n\n@cached_query(SubredditQueryCache)\ndef get_reported_links(sr_id):\n    return Link._query(Link.c.reported != 0,\n                       Link.c.sr_id == sr_id,\n                       Link.c._spam == False,\n                       sort = db_sort('new'))\n\n@cached_query(SubredditQueryCache)\ndef get_reported_comments(sr_id):\n    return Comment._query(Comment.c.reported != 0,\n                          Comment.c.sr_id == sr_id,\n                          Comment.c._spam == False,\n                          sort = db_sort('new'))\n\n@merged_cached_query\ndef get_reported(sr, user=None, include_links=True, include_comments=True):\n    sr_ids = moderated_srids(sr, user)\n    queries = []\n\n    if include_links:\n        queries.append(get_reported_links)\n    if include_comments:\n        queries.append(get_reported_comments)\n    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]\n\n@cached_query(SubredditQueryCache)\ndef get_unmoderated_links(sr_id):\n    q = Link._query(Link.c.sr_id == sr_id,\n                    Link.c._spam == (True, False),\n                    sort = db_sort('new'))\n\n    # Doesn't really work because will not return Links with no verdict\n    q._filter(or_(and_(Link.c._spam == True, Link.c.verdict != 'mod-removed'),\n                  and_(Link.c._spam == False, Link.c.verdict != 'mod-approved')))\n    return q\n\n@merged_cached_query\ndef get_modqueue(sr, user=None, include_links=True, include_comments=True):\n    sr_ids = moderated_srids(sr, user)\n    queries = []\n\n    if include_links:\n        queries.append(get_reported_links)\n        queries.append(get_spam_filtered_links)\n    if include_comments:\n        queries.append(get_reported_comments)\n        queries.append(get_spam_filtered_comments)\n    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]\n\n@merged_cached_query\ndef get_unmoderated(sr, user=None):\n    sr_ids = moderated_srids(sr, user)\n    queries = [get_unmoderated_links]\n    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]\n\ndef get_domain_links(domain, sort, time):\n    from r2.lib.db import operators\n    q = Link._query(operators.domain(Link.c.url) == filters._force_utf8(domain),\n                    sort = db_sort(sort),\n                    data = True)\n    if time != \"all\":\n        q._filter(db_times[time])\n\n    return make_results(q)\n\ndef user_query(kind, user_id, sort, time):\n    \"\"\"General profile-page query.\"\"\"\n    q = kind._query(kind.c.author_id == user_id,\n                    kind.c._spam == (True, False),\n                    sort = db_sort(sort))\n    if time != 'all':\n        q._filter(db_times[time])\n    return make_results(q)\n\ndef get_all_comments():\n    \"\"\"the master /comments page\"\"\"\n    q = Comment._query(sort = desc('_date'))\n    return make_results(q)\n\ndef get_sr_comments(sr):\n    return _get_sr_comments(sr._id)\n\ndef _get_sr_comments(sr_id):\n    \"\"\"the subreddit /r/foo/comments page\"\"\"\n    q = Comment._query(Comment.c.sr_id == sr_id,\n                       sort = desc('_date'))\n    return make_results(q)\n\ndef _get_comments(user_id, sort, time):\n    return user_query(Comment, user_id, sort, time)\n\ndef get_comments(user, sort, time):\n    return _get_comments(user._id, sort, time)\n\ndef _get_submitted(user_id, sort, time):\n    return user_query(Link, user_id, sort, time)\n\ndef get_submitted(user, sort, time):\n    return _get_submitted(user._id, sort, time)\n\n\ndef get_user_actions(user, sort, time):\n    results = []\n    unique_ids = set()\n\n    # Order is important as a listing will only have the action_type of the\n    # first occurrance (aka: posts trump comments which trump likes)\n    actions_by_type = ((get_submitted(user, sort, time), 'submit'),\n                       (get_comments(user, sort, time), 'comment'),\n                       (get_liked(user), 'like'))\n\n    for cached_result, action_type in actions_by_type:\n        cached_result.fetch()\n        for thing in cached_result.data:\n            if thing[0] not in unique_ids:\n                results.append(thing + (action_type,))\n                unique_ids.add(thing[0])\n\n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n\ndef get_overview(user, sort, time):\n    return merge_results(get_comments(user, sort, time),\n                         get_submitted(user, sort, time))\n\ndef rel_query(rel, thing_id, name, filters = []):\n    \"\"\"General relationship query.\"\"\"\n\n    q = rel._query(rel.c._thing1_id == thing_id,\n                   rel.c._t2_deleted == False,\n                   rel.c._name == name,\n                   sort = desc('_date'),\n                   eager_load = True,\n                   )\n    if filters:\n        q._filter(*filters)\n\n    return q\n\ncached_userrel_query = cached_query(UserQueryCache, filter_thing2)\ncached_srrel_query = cached_query(SubredditQueryCache, filter_thing2)\n\n@cached_userrel_query\ndef get_liked(user):\n    return FakeQuery(sort=[desc(\"_date\")])\n\n@cached_userrel_query\ndef get_disliked(user):\n    return FakeQuery(sort=[desc(\"_date\")])\n\n@cached_query(UserQueryCache)\ndef get_hidden_links(user_id):\n    return FakeQuery(sort=[desc(\"action_date\")])\n\ndef get_hidden(user):\n    return get_hidden_links(user)\n\n@cached_query(UserQueryCache)\ndef get_categorized_saved_links(user_id, sr_id, category):\n    return FakeQuery(sort=[desc(\"action_date\")])\n\n@cached_query(UserQueryCache)\ndef get_categorized_saved_comments(user_id, sr_id, category):\n    return FakeQuery(sort=[desc(\"action_date\")])\n\n@cached_query(UserQueryCache)\ndef get_saved_links(user_id, sr_id):\n    return FakeQuery(sort=[desc(\"action_date\")])\n\n@cached_query(UserQueryCache)\ndef get_saved_comments(user_id, sr_id):\n    return FakeQuery(sort=[desc(\"action_date\")])\n\ndef get_saved(user, sr_id=None, category=None):\n    sr_id = sr_id or 'none'\n    if not category:\n        queries = [get_saved_links(user, sr_id),\n                   get_saved_comments(user, sr_id)]\n    else:\n        queries = [get_categorized_saved_links(user, sr_id, category),\n                   get_categorized_saved_comments(user, sr_id, category)]\n    return MergedCachedQuery(queries)\n\n@cached_srrel_query\ndef get_subreddit_messages(sr):\n    return rel_query(ModeratorInbox, sr, 'inbox')\n\n@cached_srrel_query\ndef get_unread_subreddit_messages(sr):\n    return rel_query(ModeratorInbox, sr, 'inbox',\n                          filters = [ModeratorInbox.c.new == True])\n\ndef get_unread_subreddit_messages_multi(srs):\n    if not srs:\n        return []\n    queries = [get_unread_subreddit_messages(sr) for sr in srs]\n    return MergedCachedQuery(queries)\n\ninbox_message_rel = Inbox.rel(Account, Message)\n@cached_userrel_query\ndef get_inbox_messages(user):\n    return rel_query(inbox_message_rel, user, 'inbox')\n\n@cached_userrel_query\ndef get_unread_messages(user):\n    return rel_query(inbox_message_rel, user, 'inbox',\n                          filters = [inbox_message_rel.c.new == True])\n\ninbox_comment_rel = Inbox.rel(Account, Comment)\n@cached_userrel_query\ndef get_inbox_comments(user):\n    return rel_query(inbox_comment_rel, user, 'inbox')\n\n@cached_userrel_query\ndef get_unread_comments(user):\n    return rel_query(inbox_comment_rel, user, 'inbox',\n                          filters = [inbox_comment_rel.c.new == True])\n\n@cached_userrel_query\ndef get_inbox_selfreply(user):\n    return rel_query(inbox_comment_rel, user, 'selfreply')\n\n@cached_userrel_query\ndef get_unread_selfreply(user):\n    return rel_query(inbox_comment_rel, user, 'selfreply',\n                          filters = [inbox_comment_rel.c.new == True])\n\n\n@cached_userrel_query\ndef get_inbox_comment_mentions(user):\n    return rel_query(inbox_comment_rel, user, \"mention\")\n\n\n@cached_userrel_query\ndef get_unread_comment_mentions(user):\n    return rel_query(inbox_comment_rel, user, \"mention\",\n                     filters=[inbox_comment_rel.c.new == True])\n\n\ndef get_inbox(user):\n    return merge_results(get_inbox_comments(user),\n                         get_inbox_messages(user),\n                         get_inbox_comment_mentions(user),\n                         get_inbox_selfreply(user))\n\n@cached_query(UserQueryCache)\ndef get_sent(user_id):\n    return Message._query(Message.c.author_id == user_id,\n                          Message.c._spam == (True, False),\n                          sort = desc('_date'))\n\ndef get_unread_inbox(user):\n    return merge_results(get_unread_comments(user),\n                         get_unread_messages(user),\n                         get_unread_comment_mentions(user),\n                         get_unread_selfreply(user))\n\ndef _user_reported_query(user_id, thing_cls):\n    rel_cls = Report.rel(Account, thing_cls)\n    return rel_query(rel_cls, user_id, ('-1', '0', '1'))\n    # -1: rejected report\n    # 0: unactioned report\n    # 1: accepted report\n\n@cached_userrel_query\ndef get_user_reported_links(user_id):\n    return _user_reported_query(user_id, Link)\n\n@cached_userrel_query\ndef get_user_reported_comments(user_id):\n    return _user_reported_query(user_id, Comment)\n\n@cached_userrel_query\ndef get_user_reported_messages(user_id):\n    return _user_reported_query(user_id, Message)\n\n@merged_cached_query\ndef get_user_reported(user_id):\n    return [get_user_reported_links(user_id),\n            get_user_reported_comments(user_id),\n            get_user_reported_messages(user_id)]\n\n\ndef set_promote_status(link, promote_status):\n    all_queries = [promote_query(link.author_id) for promote_query in \n                   (get_unpaid_links, get_unapproved_links, \n                    get_rejected_links, get_live_links, get_accepted_links)]\n    all_queries.extend([get_all_unpaid_links(), get_all_unapproved_links(),\n                        get_all_rejected_links(), get_all_live_links(),\n                        get_all_accepted_links()])\n\n    if promote_status == PROMOTE_STATUS.unpaid:\n        inserts = [get_unpaid_links(link.author_id), get_all_unpaid_links()]\n    elif promote_status == PROMOTE_STATUS.unseen:\n        inserts = [get_unapproved_links(link.author_id),\n                   get_all_unapproved_links()]\n    elif promote_status == PROMOTE_STATUS.rejected:\n        inserts = [get_rejected_links(link.author_id), get_all_rejected_links()]\n    elif promote_status == PROMOTE_STATUS.promoted:\n        inserts = [get_live_links(link.author_id), get_all_live_links()]\n    elif promote_status in (PROMOTE_STATUS.accepted, PROMOTE_STATUS.pending,\n                            PROMOTE_STATUS.finished):\n        inserts = [get_accepted_links(link.author_id), get_all_accepted_links()]\n\n    deletes = list(set(all_queries) - set(inserts))\n    with CachedQueryMutator() as m:\n        for q in inserts:\n            m.insert(q, [link])\n        for q in deletes:\n            m.delete(q, [link])\n\n    link.promote_status = promote_status\n    link._commit()\n\n    text = \"set promote status to '%s'\" % PROMOTE_STATUS.name[promote_status]\n    PromotionLog.add(link, text)\n\n\ndef _promoted_link_query(user_id, status):\n    STATUS_CODES = {'unpaid': PROMOTE_STATUS.unpaid,\n                    'unapproved': PROMOTE_STATUS.unseen,\n                    'rejected': PROMOTE_STATUS.rejected,\n                    'live': PROMOTE_STATUS.promoted,\n                    'accepted': (PROMOTE_STATUS.accepted,\n                                 PROMOTE_STATUS.pending,\n                                 PROMOTE_STATUS.finished)}\n\n    q = Link._query(Link.c.sr_id == Subreddit.get_promote_srid(),\n                    Link.c._spam == (True, False),\n                    Link.c._deleted == (True, False),\n                    Link.c.promote_status == STATUS_CODES[status],\n                    sort=db_sort('new'))\n    if user_id:\n        q._filter(Link.c.author_id == user_id)\n    return q\n\n\n@cached_query(UserQueryCache)\ndef get_unpaid_links(user_id):\n    return _promoted_link_query(user_id, 'unpaid')\n\n\n@cached_query(UserQueryCache)\ndef get_all_unpaid_links():\n    return _promoted_link_query(None, 'unpaid')\n\n\n@cached_query(UserQueryCache)\ndef get_unapproved_links(user_id):\n    return _promoted_link_query(user_id, 'unapproved')\n\n\n@cached_query(UserQueryCache)\ndef get_all_unapproved_links():\n    return _promoted_link_query(None, 'unapproved')\n\n\n@cached_query(UserQueryCache)\ndef get_rejected_links(user_id):\n    return _promoted_link_query(user_id, 'rejected')\n\n\n@cached_query(UserQueryCache)\ndef get_all_rejected_links():\n    return _promoted_link_query(None, 'rejected')\n\n\n@cached_query(UserQueryCache)\ndef get_live_links(user_id):\n    return _promoted_link_query(user_id, 'live')\n\n\n@cached_query(UserQueryCache)\ndef get_all_live_links():\n    return _promoted_link_query(None, 'live')\n\n\n@cached_query(UserQueryCache)\ndef get_accepted_links(user_id):\n    return _promoted_link_query(user_id, 'accepted')\n\n\n@cached_query(UserQueryCache)\ndef get_all_accepted_links():\n    return _promoted_link_query(None, 'accepted')\n\n\n@cached_query(UserQueryCache)\ndef get_payment_flagged_links():\n    return FakeQuery(sort=[desc(\"_date\")])\n\n\ndef set_payment_flagged_link(link):\n    with CachedQueryMutator() as m:\n        q = get_payment_flagged_links()\n        m.insert(q, [link])\n\n\ndef unset_payment_flagged_link(link):\n    with CachedQueryMutator() as m:\n        q = get_payment_flagged_links()\n        m.delete(q, [link])\n\n\n@cached_query(UserQueryCache)\ndef get_underdelivered_campaigns():\n    return FakeQuery(sort=[desc(\"_date\")])\n\n\ndef set_underdelivered_campaigns(campaigns):\n    campaigns = tup(campaigns)\n    with CachedQueryMutator() as m:\n        q = get_underdelivered_campaigns()\n        m.insert(q, campaigns)\n\n\ndef unset_underdelivered_campaigns(campaigns):\n    campaigns = tup(campaigns)\n    with CachedQueryMutator() as m:\n        q = get_underdelivered_campaigns()\n        m.delete(q, campaigns)\n\n\n@merged_cached_query\ndef get_promoted_links(user_id):\n    queries = [get_unpaid_links(user_id), get_unapproved_links(user_id),\n               get_rejected_links(user_id), get_live_links(user_id),\n               get_accepted_links(user_id)]\n    return queries\n\n\n@merged_cached_query\ndef get_all_promoted_links():\n    queries = [get_all_unpaid_links(), get_all_unapproved_links(),\n               get_all_rejected_links(), get_all_live_links(),\n               get_all_accepted_links()]\n    return queries\n\n\n@cached_query(SubredditQueryCache, filter_fn=filter_thing)\ndef get_all_gilded_comments():\n    return FakeQuery(sort=[desc(\"date\")])\n\n\n@cached_query(SubredditQueryCache, filter_fn=filter_thing)\ndef get_all_gilded_links():\n    return FakeQuery(sort=[desc(\"date\")])\n\n\n@merged_cached_query\ndef get_all_gilded():\n    return [get_all_gilded_comments(), get_all_gilded_links()]\n\n\n@cached_query(SubredditQueryCache, filter_fn=filter_thing)\ndef get_gilded_comments(sr_id):\n    return FakeQuery(sort=[desc(\"date\")])\n\n\n@cached_query(SubredditQueryCache, filter_fn=filter_thing)\ndef get_gilded_links(sr_id):\n    return FakeQuery(sort=[desc(\"date\")])\n\n\n@merged_cached_query\ndef get_gilded(sr_ids):\n    queries = [get_gilded_links, get_gilded_comments]\n    return [query(sr_id)\n            for sr_id, query in itertools.product(tup(sr_ids), queries)]\n\n\n@cached_query(UserQueryCache, filter_fn=filter_thing)\ndef get_gilded_user_comments(user_id):\n    return FakeQuery(sort=[desc(\"date\")])\n\n\n@cached_query(UserQueryCache, filter_fn=filter_thing)\ndef get_gilded_user_links(user_id):\n    return FakeQuery(sort=[desc(\"date\")])\n\n\n@merged_cached_query\ndef get_gilded_users(user_ids):\n    queries = [get_gilded_user_links, get_gilded_user_comments]\n    return [query(user_id)\n            for user_id, query in itertools.product(tup(user_ids), queries)]\n\n\n@cached_query(UserQueryCache, filter_fn=filter_thing)\ndef get_user_gildings(user_id):\n    return FakeQuery(sort=[desc(\"date\")])\n\n\n@merged_cached_query\ndef get_gilded_user(user):\n    return [get_gilded_user_comments(user), get_gilded_user_links(user)]\n\n\ndef add_queries(queries, insert_items=None, delete_items=None, foreground=False):\n    \"\"\"Adds multiple queries to the query queue. If insert_items or\n       delete_items is specified, the query may not need to be\n       recomputed against the database.\"\"\"\n    for q in queries:\n        if insert_items and q.can_insert():\n            log.debug(\"Inserting %s into query %s\" % (insert_items, q))\n            if foreground:\n                q.insert(insert_items)\n            else:\n                worker.do(q.insert, insert_items)\n        elif delete_items and q.can_delete():\n            log.debug(\"Deleting %s from query %s\" % (delete_items, q))\n            if foreground:\n                q.delete(delete_items)\n            else:\n                worker.do(q.delete, delete_items)\n        else:\n            raise Exception(\"Cannot update query %r!\" % (q,))\n\n    # dual-write any queries that are being migrated to the new query cache\n    with CachedQueryMutator() as m:\n        new_queries = [getattr(q, 'new_query') for q in queries if hasattr(q, 'new_query')]\n\n        if insert_items:\n            for query in new_queries:\n                m.insert(query, tup(insert_items))\n\n        if delete_items:\n            for query in new_queries:\n                m.delete(query, tup(delete_items))\n\n#can be rewritten to be more efficient\ndef all_queries(fn, obj, *param_lists):\n    \"\"\"Given a fn and a first argument 'obj', calls the fn(obj, *params)\n    for every permutation of the parameters in param_lists\"\"\"\n    results = []\n    params = [[obj]]\n    for pl in param_lists:\n        new_params = []\n        for p in pl:\n            for c in params:\n                new_param = list(c)\n                new_param.append(p)\n                new_params.append(new_param)\n        params = new_params\n\n    results = [fn(*p) for p in params]\n    return results\n\n## The following functions should be called after their respective\n## actions to update the correct listings.\ndef new_link(link):\n    \"Called on the submission and deletion of links\"\n    sr = Subreddit._byID(link.sr_id)\n    author = Account._byID(link.author_id)\n\n    results = [get_links(sr, 'new', 'all')]\n    # we don't have to do hot/top/controversy because new_vote will do\n    # that\n\n    results.append(get_submitted(author, 'new', 'all'))\n\n    for domain in utils.UrlParser(link.url).domain_permutations():\n        results.append(get_domain_links(domain, 'new', \"all\"))\n\n    with CachedQueryMutator() as m:\n        if link._spam:\n            m.insert(get_spam_links(sr), [link])\n        if not (sr.exclude_banned_modqueue and author._spam):\n            m.insert(get_unmoderated_links(sr), [link])\n\n    add_queries(results, insert_items = link)\n    amqp.add_item('new_link', link._fullname)\n\n\ndef add_to_commentstree_q(comment):\n    if utils.to36(comment.link_id) in g.live_config[\"fastlane_links\"]:\n        amqp.add_item('commentstree_fastlane_q', comment._fullname)\n    elif g.shard_commentstree_queues:\n        amqp.add_item('commentstree_%d_q' % (comment.link_id % 10),\n                      comment._fullname)\n    else:\n        amqp.add_item('commentstree_q', comment._fullname)\n\n\ndef update_comment_notifications(comment, inbox_rels, mutator):\n    is_visible = not comment._deleted and not comment._spam\n\n    for inbox_rel in tup(inbox_rels):\n        inbox_owner = inbox_rel._thing1\n        unread = (is_visible and\n            getattr(inbox_rel, 'unread_preremoval', True))\n\n        if inbox_rel._name == \"inbox\":\n            query = get_inbox_comments(inbox_owner)\n        elif inbox_rel._name == \"selfreply\":\n            query = get_inbox_selfreply(inbox_owner)\n        else:\n            raise ValueError(\"wtf is \" + inbox_rel._name)\n\n        # mentions happen in butler_q\n\n        if is_visible:\n            mutator.insert(query, [inbox_rel])\n        else:\n            mutator.delete(query, [inbox_rel])\n\n        set_unread(comment, inbox_owner, unread=unread, mutator=mutator)\n\n\ndef new_comment(comment, inbox_rels):\n    author = Account._byID(comment.author_id)\n    job = [get_comments(author, 'new', 'all'),\n           get_comments(author, 'top', 'all'),\n           get_comments(author, 'controversial', 'all')]\n\n    sr = Subreddit._byID(comment.sr_id)\n\n    with CachedQueryMutator() as m:\n        if comment._deleted:\n            job_key = \"delete_items\"\n            job.append(get_sr_comments(sr))\n            job.append(get_all_comments())\n        else:\n            job_key = \"insert_items\"\n            if comment._spam:\n                m.insert(get_spam_comments(sr), [comment])\n            if (was_spam_filtered(comment) and\n                    not (sr.exclude_banned_modqueue and author._spam)):\n                m.insert(get_spam_filtered_comments(sr), [comment])\n\n            amqp.add_item('new_comment', comment._fullname)\n            add_to_commentstree_q(comment)\n\n        job_dict = { job_key: comment }\n        add_queries(job, **job_dict)\n\n        # note that get_all_comments() is updated by the amqp process\n        # r2.lib.db.queries.run_new_comments (to minimise lock contention)\n\n        if inbox_rels:\n            update_comment_notifications(comment, inbox_rels, mutator=m)\n\n\ndef delete_comment(comment):\n    add_to_commentstree_q(comment)\n\n\ndef new_subreddit(sr):\n    \"no precomputed queries here yet\"\n    amqp.add_item('new_subreddit', sr._fullname)\n\n\ndef new_vote(vote, foreground=False, timer=None):\n    user = vote._thing1\n    item = vote._thing2\n\n    if timer is None:\n        timer = SimpleSillyStub()\n\n    if vote.valid_thing and not item._spam and not item._deleted:\n        sr = item.subreddit_slow\n        results = []\n\n        author = Account._byID(item.author_id)\n        for sort in ('hot', 'top', 'controversial', 'new'):\n            if isinstance(item, Link):\n                results.append(get_submitted(author, sort, 'all'))\n            if isinstance(item, Comment):\n                results.append(get_comments(author, sort, 'all'))\n\n        if isinstance(item, Link):\n            # don't do 'new', because that was done by new_link, and\n            # the time-filtered versions of top/controversial will be\n            # done by mr_top\n            results.extend([get_links(sr, 'hot', 'all'),\n                            get_links(sr, 'top', 'all'),\n                            get_links(sr, 'controversial', 'all'),\n                            ])\n\n            parsed = utils.UrlParser(item.url)\n            if not is_subdomain(parsed.hostname, 'imgur.com'):\n                for domain in parsed.domain_permutations():\n                    for sort in (\"hot\", \"top\", \"controversial\"):\n                        results.append(get_domain_links(domain, sort, \"all\"))\n\n        add_queries(results, insert_items = item, foreground=foreground)\n\n    timer.intermediate(\"permacache\")\n    \n    if isinstance(item, Link):\n        # must update both because we don't know if it's a changed\n        # vote\n        with CachedQueryMutator() as m:\n            if vote._name == '1':\n                m.insert(get_liked(user), [vote])\n                m.delete(get_disliked(user), [vote])\n            elif vote._name == '-1':\n                m.delete(get_liked(user), [vote])\n                m.insert(get_disliked(user), [vote])\n            else:\n                m.delete(get_liked(user), [vote])\n                m.delete(get_disliked(user), [vote])\n\n\ndef new_message(message, inbox_rels, add_to_sent=True, update_modmail=True):\n    from r2.lib.comment_tree import add_message\n\n    from_user = Account._byID(message.author_id)\n\n    # check if the from_user is exempt from ever adding to sent\n    if not from_user.update_sent_messages:\n        add_to_sent = False\n\n    if message.display_author:\n        add_to_sent = False\n\n    modmail_rel_included = False\n    update_recipient = False\n    add_to_user = None\n\n    with CachedQueryMutator() as m:\n        if add_to_sent:\n            m.insert(get_sent(from_user), [message])\n\n        for inbox_rel in tup(inbox_rels):\n            to = inbox_rel._thing1\n\n            if isinstance(inbox_rel, ModeratorInbox):\n                m.insert(get_subreddit_messages(to), [inbox_rel])\n                modmail_rel_included = True\n            else:\n                m.insert(get_inbox_messages(to), [inbox_rel])\n                update_recipient = True\n                # make sure we add this message to the user's inbox\n                add_to_user = to\n\n            set_unread(message, to, unread=True, mutator=m)\n\n    update_modmail = update_modmail and modmail_rel_included\n\n    amqp.add_item('new_message', message._fullname)\n    add_message(message, update_recipient=update_recipient,\n                update_modmail=update_modmail, add_to_user=add_to_user)\n    \n    # light up the modmail icon for all other mods with mail access\n    if update_modmail:\n        mod_perms = message.subreddit_slow.moderators_with_perms()\n        mod_ids = [mod_id for mod_id, perms in mod_perms.iteritems()\n            if mod_id != from_user._id and perms.get('mail', False)]\n        moderators = Account._byID(mod_ids, data=True, return_dict=False)\n        for mod in moderators:\n            if not mod.modmsgtime:\n                mod.modmsgtime = message._date\n                mod._commit()\n\n\ndef set_unread(messages, to, unread, mutator=None):\n    # Maintain backwards compatability\n    messages = tup(messages)\n\n    if not mutator:\n        m = CachedQueryMutator()\n    else:\n        m = mutator\n\n    if isinstance(to, Subreddit):\n        for i in ModeratorInbox.set_unread(messages, unread):\n            q = get_unread_subreddit_messages(i._thing1_id)\n            if unread:\n                m.insert(q, [i])\n            else:\n                m.delete(q, [i])\n    else:\n        # All messages should be of the same type\n        # (asserted by Inbox.set_unread)\n        for i in Inbox.set_unread(messages, unread, to=to):\n            query = None\n            if isinstance(messages[0], Comment):\n                if i._name == \"inbox\":\n                    query = get_unread_comments(i._thing1_id)\n                elif i._name == \"selfreply\":\n                    query = get_unread_selfreply(i._thing1_id)\n                elif i._name == \"mention\":\n                    query = get_unread_comment_mentions(i._thing1_id)\n            elif isinstance(messages[0], Message):\n                query = get_unread_messages(i._thing1_id)\n            assert query is not None\n\n            if unread:\n                m.insert(query, [i])\n            else:\n                m.delete(query, [i])\n\n    if not mutator:\n        m.send()\n\n\ndef unread_handler(things, user, unread):\n    \"\"\"Given a user and Things of varying types, set their unread state.\"\"\"\n    sr_messages = collections.defaultdict(list)\n    comments = []\n    messages = []\n    # Group things by subreddit or type\n    for thing in things:\n        if isinstance(thing, Message):\n            if getattr(thing, 'sr_id', False):\n                sr_messages[thing.sr_id].append(thing)\n            else:\n                messages.append(thing)\n        else:\n            comments.append(thing)\n\n    if sr_messages:\n        mod_srs = Subreddit.reverse_moderator_ids(user)\n        srs = Subreddit._byID(sr_messages.keys())\n    else:\n        mod_srs = []\n\n    # Batch set items as unread\n    for sr_id, things in sr_messages.items():\n        # Remove the item(s) from the user's inbox\n        set_unread(things, user, unread)\n        if sr_id in mod_srs:\n            # Only moderators can change the read status of that\n            # message in the modmail inbox\n            sr = srs[sr_id]\n            set_unread(things, sr, unread)\n    if comments:\n        set_unread(comments, user, unread)\n    if messages:\n        set_unread(messages, user, unread)\n\n\ndef unnotify(thing, possible_recipients=None):\n    \"\"\"Given a Thing, remove any notifications to its possible recipients.\n\n    `possible_recipients` is a list of account IDs to unnotify. If not passed,\n    deduce all possible recipients and remove their notifications.\n    \"\"\"\n    from r2.lib import butler\n    error_message = (\"Unable to unnotify thing of type: %r\" % thing)\n    notification_handler(thing,\n        notify_function=butler.remove_mention_notification,\n        error_message=error_message,\n        possible_recipients=possible_recipients,\n    )\n\n\ndef renotify(thing, possible_recipients=None):\n    \"\"\"Given a Thing, reactivate notifications for possible recipients.\n\n    `possible_recipients` is a list of account IDs to renotify. If not passed,\n    deduce all possible recipients and add their notifications.\n    This is used when unspamming comments.\n    \"\"\"\n    from r2.lib import butler\n    error_message = (\"Unable to renotify thing of type: %r\" % thing)\n    notification_handler(thing,\n        notify_function=butler.readd_mention_notification,\n        error_message=error_message,\n        possible_recipients=possible_recipients,\n    )\n\n\ndef notification_handler(thing, notify_function,\n        error_message, possible_recipients=None):\n    if not possible_recipients:\n        possible_recipients = Inbox.possible_recipients(thing)\n\n    if not possible_recipients:\n        return\n\n    accounts = Account._byID(\n        possible_recipients,\n        return_dict=False,\n        ignore_missing=True,\n    )\n\n    if isinstance(thing, Comment):\n        rels = Inbox._fast_query(\n            accounts,\n            thing,\n            (\"inbox\", \"selfreply\", \"mention\"),\n        )\n\n        # if the comment has been spammed, remember the previous\n        # new value in case it becomes unspammed\n        if thing._spam:\n            for (tupl, rel) in rels.iteritems():\n                if rel:\n                    rel.unread_preremoval = rel.new\n                    rel._commit()\n\n        replies, mentions = utils.partition(\n            lambda r: r._name == \"mention\",\n            filter(None, rels.values()),\n        )\n\n        for mention in mentions:\n            notify_function(mention)\n\n        replies = list(replies)\n        if replies:\n            with CachedQueryMutator() as m:\n                update_comment_notifications(thing, replies, mutator=m)\n    else:\n        raise ValueError(error_message)\n\n\ndef _by_srid(things, srs=True):\n    \"\"\"Takes a list of things and returns them in a dict separated by\n       sr_id, in addition to the looked-up subreddits\"\"\"\n    ret = {}\n\n    for thing in tup(things):\n        if getattr(thing, 'sr_id', None) is not None:\n            ret.setdefault(thing.sr_id, []).append(thing)\n\n    if srs:\n        _srs = Subreddit._byID(ret.keys(), return_dict=True) if ret else {}\n        return ret, _srs\n    else:\n        return ret\n\n\ndef _by_author(things, authors=True):\n    ret = collections.defaultdict(list)\n\n    for thing in tup(things):\n        author_id = getattr(thing, 'author_id')\n        if author_id:\n            ret[author_id].append(thing)\n\n    if authors:\n        _authors = Account._byID(ret.keys(), return_dict=True) if ret else {}\n        return ret, _authors\n    else:\n        return ret\n\ndef _by_thing1_id(rels):\n    ret = {}\n    for rel in tup(rels):\n        ret.setdefault(rel._thing1_id, []).append(rel)\n    return ret\n\n\ndef was_spam_filtered(thing):\n    if (thing._spam and not thing._deleted and\n        getattr(thing, 'verdict', None) != 'mod-removed'):\n        return True\n    else:\n        return False\n\n\ndef delete(things):\n    query_cache_inserts, query_cache_deletes = _common_del_ban(things)\n    by_srid, srs = _by_srid(things)\n    by_author, authors = _by_author(things)\n\n    for sr_id, sr_things in by_srid.iteritems():\n        sr = srs[sr_id]\n        links = [x for x in sr_things if isinstance(x, Link)]\n        comments = [x for x in sr_things if isinstance(x, Comment)]\n\n        if links:\n            query_cache_deletes.append((get_spam_links(sr), links))\n            query_cache_deletes.append((get_spam_filtered_links(sr), links))\n            query_cache_deletes.append((get_unmoderated_links(sr_id),\n                                            links))\n            query_cache_deletes.append((get_edited_links(sr_id), links))\n        if comments:\n            query_cache_deletes.append((get_spam_comments(sr), comments))\n            query_cache_deletes.append((get_spam_filtered_comments(sr),\n                                        comments))\n            query_cache_deletes.append((get_edited_comments(sr), comments))\n\n    for author_id, a_things in by_author.iteritems():\n        author = authors[author_id]\n        links = [x for x in a_things if isinstance(x, Link)]\n        comments = [x for x in a_things if isinstance(x, Comment)]\n\n        if links:\n            results = [get_submitted(author, 'hot', 'all'),\n                       get_submitted(author, 'new', 'all')]\n            for sort in time_filtered_sorts:\n                for time in db_times.keys():\n                    results.append(get_submitted(author, sort, time))\n            add_queries(results, delete_items=links)\n            query_cache_inserts.append((get_deleted_links(author_id), links))\n        if comments:\n            results = [get_comments(author, 'hot', 'all'),\n                       get_comments(author, 'new', 'all')]\n            for sort in time_filtered_sorts:\n                for time in db_times.keys():\n                    results.append(get_comments(author, sort, time))\n            add_queries(results, delete_items=comments)\n            query_cache_inserts.append((get_deleted_comments(author_id),\n                                        comments))\n\n    with CachedQueryMutator() as m:\n        for q, inserts in query_cache_inserts:\n            m.insert(q, inserts)\n        for q, deletes in query_cache_deletes:\n            m.delete(q, deletes)\n\n    for thing in tup(things):\n        thing.update_search_index()\n\n\ndef edit(thing):\n    if isinstance(thing, Link):\n        query = get_edited_links\n    elif isinstance(thing, Comment):\n        query = get_edited_comments\n\n    with CachedQueryMutator() as m:\n        m.delete(query(thing.sr_id), [thing])\n        m.insert(query(thing.sr_id), [thing])\n\n\ndef ban(things, filtered=True):\n    query_cache_inserts, query_cache_deletes = _common_del_ban(things)\n    by_srid = _by_srid(things, srs=False)\n\n    for sr_id, sr_things in by_srid.iteritems():\n        links = []\n        modqueue_links = []\n        comments = []\n        modqueue_comments = []\n        for item in sr_things:\n            # don't add posts by banned users if subreddit prefs exclude them\n            add_to_modqueue = (filtered and\n                       not (item.subreddit_slow.exclude_banned_modqueue and\n                            item.author_slow._spam))\n\n            if isinstance(item, Link):\n                links.append(item)\n                if add_to_modqueue:\n                    modqueue_links.append(item)\n            elif isinstance(item, Comment):\n                comments.append(item)\n                if add_to_modqueue:\n                    modqueue_comments.append(item)\n\n        if links:\n            query_cache_inserts.append((get_spam_links(sr_id), links))\n            if not filtered:\n                query_cache_deletes.append(\n                        (get_spam_filtered_links(sr_id), links))\n                query_cache_deletes.append(\n                        (get_unmoderated_links(sr_id), links))\n\n        if modqueue_links:\n            query_cache_inserts.append(\n                    (get_spam_filtered_links(sr_id), modqueue_links))\n\n        if comments:\n            query_cache_inserts.append((get_spam_comments(sr_id), comments))\n            if not filtered:\n                query_cache_deletes.append(\n                        (get_spam_filtered_comments(sr_id), comments))\n\n        if modqueue_comments:\n            query_cache_inserts.append(\n                    (get_spam_filtered_comments(sr_id), modqueue_comments))\n\n    with CachedQueryMutator() as m:\n        for q, inserts in query_cache_inserts:\n            m.insert(q, inserts)\n        for q, deletes in query_cache_deletes:\n            m.delete(q, deletes)\n\n    for thing in tup(things):\n        thing.update_search_index()\n\n\ndef _common_del_ban(things):\n    query_cache_inserts = []\n    query_cache_deletes = []\n    by_srid, srs = _by_srid(things)\n\n    for sr_id, sr_things in by_srid.iteritems():\n        sr = srs[sr_id]\n        links = [x for x in sr_things if isinstance(x, Link)]\n        comments = [x for x in sr_things if isinstance(x, Comment)]\n\n        if links:\n            results = [get_links(sr, 'hot', 'all'), get_links(sr, 'new', 'all')]\n            for sort in time_filtered_sorts:\n                for time in db_times.keys():\n                    results.append(get_links(sr, sort, time))\n            add_queries(results, delete_items=links)\n            query_cache_deletes.append([get_reported_links(sr), links])\n        if comments:\n            query_cache_deletes.append([get_reported_comments(sr), comments])\n\n    return query_cache_inserts, query_cache_deletes\n\n\ndef unban(things, insert=True):\n    query_cache_deletes = []\n\n    by_srid, srs = _by_srid(things)\n    if not by_srid:\n        return\n\n    for sr_id, things in by_srid.iteritems():\n        sr = srs[sr_id]\n        links = [x for x in things if isinstance(x, Link)]\n        comments = [x for x in things if isinstance(x, Comment)]\n\n        if insert and links:\n            # put it back in the listings\n            results = [get_links(sr, 'hot', 'all'),\n                       get_links(sr, 'top', 'all'),\n                       get_links(sr, 'controversial', 'all'),\n                       ]\n            # the time-filtered listings will have to wait for the\n            # next mr_top run\n            add_queries(results, insert_items=links)\n\n            # Check if link is being unbanned and should be put in\n            # 'new' with current time\n            new_links = []\n            for l in links:\n                ban_info = l.ban_info\n                if ban_info.get('reset_used', True) == False and \\\n                    ban_info.get('auto', False):\n                    l_copy = deepcopy(l)\n                    l_copy._date = ban_info['unbanned_at']\n                    new_links.append(l_copy)\n                else:\n                    new_links.append(l)\n            add_queries([get_links(sr, 'new', 'all')], insert_items=new_links)\n            query_cache_deletes.append([get_spam_links(sr), links])\n\n        if insert and comments:\n            add_queries([get_all_comments(), get_sr_comments(sr)],\n                        insert_items=comments)\n            query_cache_deletes.append([get_spam_comments(sr), comments])\n\n        if links:\n            query_cache_deletes.append((get_unmoderated_links(sr), links))\n            query_cache_deletes.append([get_spam_filtered_links(sr), links])\n\n        if comments:\n            query_cache_deletes.append([get_spam_filtered_comments(sr), comments])\n\n    with CachedQueryMutator() as m:\n        for q, deletes in query_cache_deletes:\n            m.delete(q, deletes)\n\n    for thing in tup(things):\n        thing.update_search_index()\n\ndef new_report(thing, report_rel):\n    reporter_id = report_rel._thing1_id\n\n    with CachedQueryMutator() as m:\n        if isinstance(thing, Link):\n            m.insert(get_reported_links(thing.sr_id), [thing])\n            m.insert(get_user_reported_links(reporter_id), [report_rel])\n        elif isinstance(thing, Comment):\n            m.insert(get_reported_comments(thing.sr_id), [thing])\n            m.insert(get_user_reported_comments(reporter_id), [report_rel])\n        elif isinstance(thing, Message):\n            m.insert(get_user_reported_messages(reporter_id), [report_rel])\n\n    amqp.add_item(\"new_report\", thing._fullname)\n\n\ndef clear_reports(things, rels):\n    query_cache_deletes = []\n\n    by_srid = _by_srid(things, srs=False)\n\n    for sr_id, sr_things in by_srid.iteritems():\n        links = [ x for x in sr_things if isinstance(x, Link) ]\n        comments = [ x for x in sr_things if isinstance(x, Comment) ]\n\n        if links:\n            query_cache_deletes.append([get_reported_links(sr_id), links])\n        if comments:\n            query_cache_deletes.append([get_reported_comments(sr_id), comments])\n\n    # delete from user_reported if the report was correct\n    rels = [r for r in rels if r._name == '1']\n    if rels:\n        link_rels = [r for r in rels if r._type2 == Link]\n        comment_rels = [r for r in rels if r._type2 == Comment]\n        message_rels = [r for r in rels if r._type2 == Message]\n\n        rels_to_query = ((link_rels, get_user_reported_links),\n                         (comment_rels, get_user_reported_comments),\n                         (message_rels, get_user_reported_messages))\n\n        for thing_rels, query in rels_to_query:\n            if not thing_rels:\n                continue\n\n            by_thing1_id = _by_thing1_id(thing_rels)\n            for reporter_id, reporter_rels in by_thing1_id.iteritems():\n                query_cache_deletes.append([query(reporter_id), reporter_rels])\n\n    with CachedQueryMutator() as m:\n        for q, deletes in query_cache_deletes:\n            m.delete(q, deletes)\n\n\ndef add_all_srs():\n    \"\"\"Recalculates every listing query for every subreddit. Very,\n       very slow.\"\"\"\n    q = Subreddit._query(sort = asc('_date'))\n    for sr in fetch_things2(q):\n        for q in all_queries(get_links, sr, ('hot', 'new'), ['all']):\n            q.update()\n        for q in all_queries(get_links, sr, time_filtered_sorts, db_times.keys()):\n            q.update()\n        get_spam_links(sr).update()\n        get_spam_comments(sr).update()\n        get_reported_links(sr).update()\n        get_reported_comments(sr).update()\n\ndef update_user(user):\n    if isinstance(user, str):\n        user = Account._by_name(user)\n    elif isinstance(user, int):\n        user = Account._byID(user)\n\n    results = [get_inbox_messages(user),\n               get_inbox_comments(user),\n               get_inbox_selfreply(user),\n               get_sent(user),\n               get_liked(user),\n               get_disliked(user),\n               get_submitted(user, 'new', 'all'),\n               get_comments(user, 'new', 'all')]\n    for q in results:\n        q.update()\n\ndef add_all_users():\n    q = Account._query(sort = asc('_date'))\n    for user in fetch_things2(q):\n        update_user(user)\n\n# amqp queue processing functions\n\ndef run_new_comments(limit=1000):\n    \"\"\"Add new incoming comments to the /comments page\"\"\"\n    # this is done as a queue because otherwise the contention for the\n    # lock on the query would be very high\n\n    @g.stats.amqp_processor('newcomments_q')\n    def _run_new_comments(msgs, chan):\n        fnames = [msg.body for msg in msgs]\n\n        comments = Comment._by_fullname(fnames, data=True, return_dict=False)\n        add_queries([get_all_comments()],\n                    insert_items=comments)\n\n        bysrid = _by_srid(comments, False)\n        for srid, sr_comments in bysrid.iteritems():\n            add_queries([_get_sr_comments(srid)],\n                        insert_items=sr_comments)\n\n    amqp.handle_items('newcomments_q', _run_new_comments, limit=limit)\n\ndef run_commentstree(qname=\"commentstree_q\", limit=100):\n    \"\"\"Add new incoming comments to their respective comments trees\"\"\"\n\n    @g.stats.amqp_processor(qname)\n    def _run_commentstree(msgs, chan):\n        comments = Comment._by_fullname([msg.body for msg in msgs],\n                                        data = True, return_dict = False)\n        print 'Processing %r' % (comments,)\n\n        # when fastlaning a thread, we may need to have this qproc ignore\n        # messages that were put into the non-fastlane queue and are causing\n        # both to back up. a full recompute of the old thread will fix these\n        # missed messages.\n        if qname != \"commentstree_fastlane_q\":\n            fastlaned_links = g.live_config[\"fastlane_links\"]\n            links = Link._byID([com.link_id for com in comments], data=True)\n            comments = [com for com in comments\n                        if utils.to36(com.link_id) not in fastlaned_links and\n                           links[com.link_id].skip_commentstree_q != qname]\n\n        if comments:\n            add_comments(comments)\n\n    amqp.handle_items(qname, _run_commentstree, limit = limit)\n\nvote_link_q = 'vote_link_q'\nvote_comment_q = 'vote_comment_q'\nvote_fastlane_q = 'vote_fastlane_q'\n\nvote_names_by_dir = {True: \"1\", None: \"0\", False: \"-1\"}\nvote_dirs_by_name = {v: k for k, v in vote_names_by_dir.iteritems()}\n\ndef queue_vote(user, thing, dir, ip, vote_info=None, cheater=False, store=True,\n               event_data=None):\n    # set the vote in memcached so the UI gets updated immediately\n    key = prequeued_vote_key(user, thing)\n    grace_period = int(g.vote_queue_grace_period.total_seconds())\n    g.cache.set(key, vote_names_by_dir[dir], time=grace_period+1)\n\n    # update LastModified immediately to help us cull prequeued_vote lookups\n    rel_cls = VotesByAccount.rel(user.__class__, thing.__class__)\n    LastModified.touch(user._fullname, rel_cls._last_modified_name)\n\n    # queue the vote to be stored unless told not to\n    if store:\n        if isinstance(thing, Link):\n            if thing._id36 in g.live_config[\"fastlane_links\"]:\n                qname = vote_fastlane_q\n            else:\n                if g.shard_link_vote_queues:\n                    qname = \"vote_link_%s_q\" % str(thing.sr_id)[-1]\n                else:\n                    qname = vote_link_q\n\n        elif isinstance(thing, Comment):\n            if utils.to36(thing.link_id) in g.live_config[\"fastlane_links\"]:\n                qname = vote_fastlane_q\n            else:\n                qname = vote_comment_q\n        else:\n            log.warning(\"%s tried to vote on %r. that's not a link or comment!\",\n                        user, thing)\n            return\n\n        vote = {\n            \"uid\": user._id,\n            \"tid\": thing._fullname,\n            \"dir\": dir,\n            \"ip\": ip,\n            \"info\": vote_info,\n            \"cheater\": cheater,\n            \"event\": event_data,\n        }\n        amqp.add_item(qname, json.dumps(vote))\n\ndef prequeued_vote_key(user, item):\n    return 'registered_vote_%s_%s' % (user._id, item._fullname)\n\n\ndef _by_type(items):\n    by_type = collections.defaultdict(list)\n    for item in items:\n        by_type[item.__class__].append(item)\n    return by_type\n\n\ndef get_likes(user, requested_items):\n    if not user or not requested_items:\n        return {}\n\n    res = {}\n\n    try:\n        last_modified = LastModified._byID(user._fullname)\n    except tdb_cassandra.NotFound:\n        last_modified = None\n\n    items_in_grace_period = {}\n    items_by_type = _by_type(requested_items)\n    for type_, items in items_by_type.iteritems():\n        try:\n            rel_cls = VotesByAccount.rel(user.__class__, type_)\n        except tdb_cassandra.TdbException:\n            # these items can't be voted on. just mark 'em as None and skip.\n            for item in items:\n                res[(user, item)] = None\n            continue\n\n        last_vote = getattr(last_modified, rel_cls._last_modified_name, None)\n        if last_vote:\n            time_since_last_vote = datetime.now(pytz.UTC) - last_vote\n\n        # only do prequeued_vote lookups if we've voted within the grace period\n        # and therefore might have votes in flight in the queues.\n        if last_vote and time_since_last_vote < g.vote_queue_grace_period:\n            too_new = 0\n\n            for item in items:\n                if item._age > time_since_last_vote:\n                    key = prequeued_vote_key(user, item)\n                    items_in_grace_period[key] = (user, item)\n                else:\n                    # the item is newer than our last vote, we can't have\n                    # possibly voted on it.\n                    res[(user, item)] = None\n                    too_new += 1\n\n            if too_new:\n                g.stats.simple_event(\"vote.prequeued.too-new\", delta=too_new)\n        else:\n            g.stats.simple_event(\"vote.prequeued.graceless\", delta=len(items))\n\n    # look up votes in memcache for items that could have been voted on\n    # but not processed by a queue processor yet.\n    if items_in_grace_period:\n        g.stats.simple_event(\n            \"vote.prequeued.fetch\", delta=len(items_in_grace_period))\n        r = g.cache.get_multi(items_in_grace_period.keys())\n        for key, v in r.iteritems():\n            res[items_in_grace_period[key]] = vote_dirs_by_name[v]\n\n    cassavotes = get_votes(\n        user, [i for i in requested_items if (user, i) not in res])\n    res.update(cassavotes)\n\n    return res\n\n\ndef handle_vote(user, thing, vote, foreground=False, timer=None, date=None):\n    if timer is None:\n        timer = SimpleSillyStub()\n\n    from r2.lib.db import tdb_sql\n    from sqlalchemy.exc import IntegrityError\n    try:\n        v = cast_vote(user, thing, vote, timer=timer, date=date)\n    except (tdb_sql.CreationError, IntegrityError):\n        g.log.error(\"duplicate vote for: %s\" % str((user, thing, dir)))\n        return\n\n    new_vote(v, foreground=foreground, timer=timer)\n\n    timestamps = []\n    if isinstance(thing, Link):\n\n        #update the modified flags\n        if user._id == thing.author_id:\n            timestamps.append('Overview')\n            timestamps.append('Submitted')\n            #update sup listings\n            sup.add_update(user, 'submitted')\n\n            #update sup listings\n            if dir:\n                sup.add_update(user, 'liked')\n            elif dir is False:\n                sup.add_update(user, 'disliked')\n\n    elif isinstance(thing, Comment):\n        #update last modified\n        if user._id == thing.author_id:\n            timestamps.append('Overview')\n            timestamps.append('Commented')\n            #update sup listings\n            sup.add_update(user, 'commented')\n\n    else:\n        raise NotImplementedError\n\n    timer.intermediate(\"sup\")\n\n    for timestamp in timestamps:\n        set_last_modified(user, timestamp.lower())\n    LastModified.touch(user._fullname, timestamps)\n    timer.intermediate(\"last_modified\")\n\n\ndef process_votes(qname, limit=0):\n    # limit is taken but ignored for backwards compatibility\n    stats_qname = qname\n    if stats_qname.startswith(\"vote_link\"):\n        stats_qname = \"vote_link_q\"\n\n    @g.stats.amqp_processor(stats_qname)\n    def _handle_vote(msg):\n        timer = stats.get_timer(\"service_time.\" + stats_qname)\n        timer.start()\n\n        vote = json.loads(msg.body)\n\n        voter = Account._byID(vote[\"uid\"], data=True)\n        votee = Thing._by_fullname(vote[\"tid\"], data=True)\n        timer.intermediate(\"preamble\")\n\n        # Convert the naive timestamp we got from amqplib to a\n        # timezone aware one.\n        tt = mktime(msg.timestamp.timetuple())\n        date = datetime.utcfromtimestamp(tt).replace(tzinfo=pytz.UTC)\n\n        # I don't know how, but somebody is sneaking in votes\n        # for subreddits\n        if isinstance(votee, (Link, Comment)):\n            print (voter, votee, vote[\"dir\"], vote[\"ip\"], vote[\"info\"],\n                   vote[\"cheater\"])\n            handle_vote(voter, votee, vote, foreground=True, timer=timer,\n                        date=date)\n\n        if isinstance(votee, Comment):\n            update_comment_votes([votee])\n            timer.intermediate(\"update_comment_votes\")\n\n        stats.simple_event('vote.total')\n        if vote[\"cheater\"]:\n            stats.simple_event('vote.cheater')\n        timer.flush()\n\n    amqp.consume_items(qname, _handle_vote, verbose = False)\n\n\ndef consume_mark_all_read():\n    @g.stats.amqp_processor('markread_q')\n    def process_mark_all_read(msg):\n        user = Account._by_fullname(msg.body)\n        inbox_fullnames = get_unread_inbox(user)\n        for inbox_chunk in in_chunks(inbox_fullnames, size=100):\n            things = Thing._by_fullname(inbox_chunk, return_dict=False)\n            unread_handler(things, user, unread=False)\n\n    amqp.consume_items('markread_q', process_mark_all_read)\n\n\ndef consume_deleted_accounts():\n    @g.stats.amqp_processor('del_account_q')\n    def process_deleted_accounts(msg):\n        account = Thing._by_fullname(msg.body)\n        assert isinstance(account, Account)\n\n        if account.has_stripe_subscription:\n            from r2.controllers.ipn import cancel_stripe_subscription\n            cancel_stripe_subscription(account.gold_subscr_id)\n\n        # Mark their link submissions for updating on cloudsearch\n        query = LinksByAccount._cf.xget(account._id36)\n        for link_id36, unused in query:\n            fullname = Link._fullname_from_id36(link_id36)\n            msg = pickle.dumps({\"fullname\": fullname})\n            amqp.add_item(\"search_changes\", msg, message_id=fullname,\n                delivery_mode=amqp.DELIVERY_TRANSIENT)\n\n    amqp.consume_items('del_account_q', process_deleted_accounts)\n",
			"file": "/D/Git/reddit/r2/r2/lib/db/queries.py",
			"file_size": 69883,
			"file_write_time": 130795985510055586,
			"settings":
			{
				"buffer_size": 67915,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/lib/pages/things.py",
			"settings":
			{
				"buffer_size": 14006,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Git/reddit/r2/r2/controllers/api.py",
			"settings":
			{
				"buffer_size": 177066,
				"line_ending": "Windows"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 81.0,
		"last_filter": "Package Control: inst",
		"selected_items":
		[
			[
				"Package Control: inst",
				"Package Control: Install Package"
			],
			[
				"Package Control: insta",
				"Package Control: Install Package"
			],
			[
				"Package Control: in",
				"Package Control: Install Package"
			],
			[
				"Package Control: ins",
				"Package Control: Install Package"
			]
		],
		"width": 512.0
	},
	"console":
	{
		"height": 146.0,
		"history":
		[
			"import urllib.request,os,hashlib; h = 'eb2297e1a458f27d836c04bb0cbaf282' + 'd0e7a3098092775ccb37ca9d6b2e4b7d'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)",
			"import urllib.request,os; pr='Preferences.sublime-settings'; ip='ignored_packages'; n='Package Control'; s=sublime.load_settings(pr); ig=s.get(ip); ig.append(n); s.set(ip,ig); sublime.save_settings('Preferences.sublime-settings'); pf=n+'.sublime-package'; urllib.request.install_opener(urllib.request.build_opener(urllib.request.ProxyHandler())); by=urllib.request.urlopen('https://packagecontrol.io/'+pf.replace(' ','%20')).read(); open(os.path.join(sublime.installed_packages_path(),pf),'wb').write(by); ig.remove(n); s.set(ip,ig); sublime.save_settings(pr); print('Package Control: 3.0.0 upgrade successful!')",
			"import urllib.request,os,hashlib; h = '7183a2d3e96f11eeadd761d777e62404' + 'e330c659d4bb41d3bdf022e94cab3cd0'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/D/Git/reddit",
		"/D/Git/reddit/r2",
		"/D/Git/reddit/r2/r2",
		"/D/Git/reddit/r2/r2/lib",
		"/D/Git/reddit/sql",
		"/D/Git/Skimur"
	],
	"file_history":
	[
		"/D/Git/reddit/r2/r2/lib/cache.py",
		"/D/Git/reddit/r2/r2/models/subreddit.py",
		"/D/Git/reddit/r2/r2/lib/db/tdb_cassandra.py",
		"/D/Git/reddit/.idea/workspace.xml",
		"/D/Git/reddit/r2/r2/lib/utils/_utils.pyx",
		"/D/Git/reddit/r2/r2/lib/db/thing.py",
		"/D/Git/reddit/r2/r2/lib/comment_tree.py",
		"/D/Git/reddit/r2/r2/controllers/reddit_base.py",
		"/D/Git/reddit/install-reddit.sh",
		"/D/Git/reddit/r2/setup.py",
		"/D/Git/reddit/r2/r2/controllers/front.py",
		"/D/Git/Skimur/upp.log",
		"/D/Git/reddit/r2/r2/lib/db/tdb_sql.py",
		"/D/Git/reddit-vagrant/Vagrantfile",
		"/D/Git/reddit-vagrant/bootstrap.sh",
		"/D/Git/reddit/r2/r2/lib/js.py",
		"/D/Git/reddit/r2/r2/lib/db/queries.py",
		"/D/Git/reddit/r2/r2/lib/db/_sorts.pyx",
		"/D/Git/reddit/r2/r2/controllers/listingcontroller.py",
		"/D/Git/reddit-vagrant/vagrant_config.yml.devexample",
		"/C/Windows/System32/drivers/etc/hosts",
		"/D/Git/reddit/r2/r2/lib/providers/search/__init__.py",
		"/D/Git/skimur.github.io/old/index.html",
		"/D/Git/skimur.github.io/js/npm.js",
		"/D/Git/skimur.github.io/stylesheets/stylesheet.css",
		"/D/Git/reddit/r2/r2/lib/pages/pages.py",
		"/D/Git/Skimur/Vagrantfile",
		"/D/Git/skimur.github.io/CNAME",
		"/D/Git/skimur.github.io/index.html",
		"/D/Git/reddit/r2/r2/lib/wrapped.pyx",
		"/D/Git/reddit/r2/r2/templates/linklisting.html",
		"/D/Git/reddit/r2/r2/templates/listing.html",
		"/D/Git/Skimur/README.md",
		"/C/Users/Paul/Desktop/variables (3).less",
		"/C/Users/Paul/Desktop/bootswatch (1).less",
		"/D/Git/Skimur/LICENSE",
		"/C/Users/Paul/Downloads/Adobe Illustrator CS6 16.0.0 (32-64 bit) [ChingLiu]/How To Open Nfo Files.txt",
		"/D/Git/Yenguin/src/packages/repositories.config",
		"/D/Git/Yenguin/puppet/manifests/default.pp",
		"/D/Git/Yenguin/Vagrantfile",
		"/D/Git/Yenguin/src/Infrastructure/Infrastructure/obj/Debug/Infrastructure.csproj.FileListAbsolute.txt",
		"/D/Git/Yenguin/src/.vs/config/applicationhost.config",
		"/D/Git/reddit/r2/r2/models/admintools.py",
		"/D/Git/reddit/r2/r2/lib/utils/utils.py",
		"/D/Git/reddit/r2/r2/models/link.py",
		"/D/Git/reddit/r2/r2/lib/validator/validator.py",
		"/D/Git/reddit/r2/r2/controllers/promotecontroller.py",
		"/D/Git/reddit/r2/r2/models/account.py",
		"/D/Git/reddit/r2/r2/lib/automoderator.py",
		"/D/Git/reddit/r2/r2/controllers/api.py",
		"/D/Git/Yenguin/puppet/modules/redis/README.md",
		"/D/Git/Yenguin/database/createdatabase.sql",
		"/D/Git/Yenguin/src/Infrastructure/Infrastructure.Messaging.MassTransit/MassTransit.cs",
		"/D/Git/Yenguin/.editorconfig",
		"/D/Git/Yenguin/.gitattributes",
		"/D/Git/Yenguin/modules/apt/README.md",
		"/D/Git/Yenguin/manifests/default.pp",
		"/D/Git/Yenguin/modules/apt/examples/builddep.pp",
		"/D/Git/Yenguin/modules/apt/examples/unattended_upgrades.pp",
		"/D/Git/Yenguin/src/Vagrantfile",
		"/D/Git/Yenguin/provisioning/install-puppet.sh",
		"/C/ProgramData/PuppetLabs/puppet/etc/puppet.conf",
		"/D/Git/Yenguin/provisioning/provision.sh",
		"/D/Git/Yenguin/provisioning/aur.sh",
		"/C/Users/Paul/Desktop/jquery-2.1.4.js",
		"/D/Git/Yenguin/src/Yenguin.Web.Public/Controllers/ManageController.cs",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI39.680/bootstrap.js",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI53.680/npm.js",
		"/C/Users/Paul/Desktop/jquery-1.11.3.js",
		"/C/Users/Paul/AppData/Local/Microsoft/VisualStudio/12.0/Extensions/tnykbj5u.o0b/Resources/nodejs/site.css",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/_start/plugins/bootstrap/css/bootstrap.css.map",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/assets/global/scripts/datatable.js",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/assets/global/scripts/metronic.js",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/assets/global/plugins/jquery.input-ip-address-control-1.0.min.js",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/templates/admin_material_design/login.html",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/assets/global/plugins/bootstrap-datepaginator/README.md",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/templates/admin_material_design/index_2.html",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme_rtl/assets/global/css/components.css.map",
		"/C/Users/Paul/Downloads/KMSpico 9.1.3 Final/ReadMe KMSpico Install.txt",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI28.296/_navs.scss",
		"/C/Users/Paul/Downloads/Windows 8.1 KMS Activator Ultimate v1.5 {AmanPC}/Instructions.txt",
		"/D/Git/voat/Whoaverse/Whoaverse/Utils/Voting.cs",
		"/D/Git/voat/Whoaverse/Whoaverse/Utils/Ranking.cs",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/sass/koala-config.json",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/sass/global/components/_wells.scss",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/theme/sass/global/components/_modals.scss",
		"/D/Git/Yenguin/src/temptheme/v4.0.2/_documentation/admin/index.html",
		"/D/Git/voat/Whoaverse/UnitTests/UnitTests.cs",
		"/D/Git/Yenguin/v4.0.2/_documentation/admin/index.html",
		"/D/Git/Yenguin/v4.0.2/theme/templates/admin_material_design/index_2.html",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI40.528/koala-config.json",
		"/D/Git/FFMPEG-Windows-Build/default.ps1",
		"/D/Git/cqrs-journey/install-packages.ps1",
		"/C/Users/Paul/Documents/IISExpress/config/applicationhost.config",
		"/D/Git/voat/Web.config",
		"/D/Git/voat/Whoaverse/Whoaverse/Controllers/HomeController.cs",
		"/C/Users/Paul/Documents/IISExpress/config/aspnet.config",
		"/C/Users/Paul/Documents/IISExpress/config/redirection.config",
		"/C/Users/Paul/Desktop/test.html",
		"/D/Git/ServiceStack/build.cmd",
		"/D/Git/Enciris/Src/EncirisPlayer/LTSDLRenderer.cpp",
		"//192.168.5.5/RnD/DSSP1_DRSHD1080_1/$OEM$ Folders/MedXChange_DRSHD1080p/$OEM$/$1/Install/Unattend/Sysprep Notes.txt",
		"/C/.dnx/bin/dnvm.ps1",
		"/C/Users/Paul/.dnx/bin/dnvm.ps1",
		"/C/Users/Paul/Downloads/OS X Mavericks 10.9 Retail VMware Image/7z-extracter.cmd",
		"/C/Users/Paul/Downloads/OS X Mavericks 10.9 Retail VMware Image/OS X Mavericks 10.9 Retail VMware Image/OS X Mavericks/OS X Mavericks.vmx",
		"/D/Git/dcmtk-source/dcmnet/include/dcmtk/dcmnet/scppool.h",
		"/D/Git/dcmtk-source/dcmwlm/libsrc/wlfsim.cc",
		"/D/Git/dcmtk-source/dcmwlm/include/dcmtk/dcmwlm/wldsfs.h",
		"/D/Git/dcmtk-source/dcmnet/libsrc/scppool.cc",
		"/D/Git/MedXChange.DCMTK/Src/Dependencies/dcmtk-3.6.1-a05b7b9-win32/include/dcmtk/dcmnet/scp.h",
		"/D/Git/MedXChange.DCMTK/Src/Dependencies/dcmtk-3.6.1-a05b7b9-win32/include/dcmtk/dcmsr/dsrdoc.h",
		"/D/Git/MedXChange.DCMTK/Src/MedXChange.DCMTK.Cons/bin/Debug/logger.cfg",
		"/D/Git/MedXBroker/Src/Dependencies/Mirth Connect/logs/mirth.log",
		"/D/Git/MedXBroker/Src/Dependencies/Source/test.txt",
		"/C/Users/Paul/Desktop/Example - CSV to JSON POST (1).xml",
		"/D/Git/MedXBroker/Src/Dependencies/transform.js",
		"/D/Git/MedXBroker/Src/Dependencies/test.txt",
		"/C/Users/Paul/AppData/Local/Temp/tmpB3C7.tmp.h",
		"/C/Users/Paul/AppData/Local/Temp/tmpA2BC.tmp.cpp",
		"/D/Git/DCMTK-build/dcmtk-output/share/dcmtk/wlistdb/OFFIS/wklist1.dump",
		"/D/Git/DCMTK-build/dcmtk-output/share/dcmtk/wlistdb/README",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI95.696/browserconfig.xml",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI18.696/main.js",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI56.696/npm.js",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI69.696/main.css",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI89.696/plugins.js",
		"/C/Users/Paul/AppData/Local/Temp/Rar$DI72.696/main.js"
	],
	"find":
	{
		"height": 43.0
	},
	"find_in_files":
	{
		"height": 90.0,
		"where_history":
		[
			"",
			"reddit",
			"",
			"scp.cc",
			"",
			"D:\\Git\\MedXChange.DCMTK\\Src\\Dependencies\\dcmtk-3.6.1-a05b7b9-win32\\include\\dcmtk",
			"*.h",
			"",
			"*.cs",
			"",
			"*.cs",
			"",
			"D:\\Git\\Enciris\\Src\\Dependencies\\SDL2-2.0.3",
			"",
			"D:\\Git\\Enciris\\Src\\Dependencies\\enciris",
			"D:\\Git\\Enciris\\Src\\Dependencies\\FFMPEG",
			"",
			"D:\\Git\\Enciris\\Src\\Dependencies\\FFMPEG\\doc\\examples",
			"",
			"*.cpp",
			""
		]
	},
	"find_state":
	{
		"case_sensitive": true,
		"find_history":
		[
			"normalized_rising",
			"sgm",
			"normalized_rising",
			"rising",
			"calc_rising",
			"set_rising",
			"calc_rising",
			"rising",
			"reddit-job-rising",
			"rising",
			"create index",
			"normalized",
			"def hot",
			"normalize_hot",
			"_hot",
			"_sorts",
			"sorts",
			"def _hot",
			"_hot",
			"cpdef double _hot",
			"db_sort",
			"_get_links",
			"def get_links",
			"get_links",
			"def get_links",
			"get_links",
			"'hot'",
			"hot",
			"ageweight",
			"weight",
			"age",
			"ageweight",
			"normalized_hot",
			"get_hot_tuples",
			"fast_query",
			"thing_cache",
			"disallow_db_writes",
			"cassandra",
			"_connection_pool",
			"cassandra",
			"haproxy",
			"cassandra",
			"_hoty",
			"week",
			"def timeago",
			"timeago",
			"week",
			"past week",
			"all",
			"timeFilter",
			"_sorts",
			"The hot formula. Should match the equivalent function in postgres",
			"_get_links",
			"fetch_multi",
			"NLog",
			"page_cache_time",
			"servers",
			"get_multi",
			"CachedResults",
			"make_results",
			"_get_links",
			"_incr",
			"def _query",
			"class Link",
			"def _query",
			"_query",
			"_get_links",
			"query",
			"class Link",
			"_get_links",
			"class",
			"def Link",
			"Link",
			"db_sort",
			"_get_links",
			"generator",
			"link",
			"test",
			"setup.py",
			"postgres",
			"email_db",
			"comment_db",
			"main_db",
			"HOT_PAGE_AGE",
			"normalized_hot",
			"fetch_multi",
			"effective_hot",
			"get_hot_factor",
			"get_hot_tuples",
			"normalized_hot",
			"link",
			"after",
			"links",
			"hot_links_by_url_listing",
			"links",
			"cache_ignore",
			"add_props",
			"rendercache",
			"rendercaches",
			"memcache",
			"LinksByUrl",
			"tdb_cassandra",
			"cassandra",
			"tbd_cassandra",
			"submit_event",
			"update_search_index",
			"submit",
			"submit post",
			"make_item_tuple",
			"controversial",
			"LinkListing",
			"listing",
			"CachedQuery",
			"_CachedQueryBase",
			"CachedQuery",
			"query_cache",
			"fetch_multi",
			"get_hot_factor",
			"normalized_hot",
			"hot",
			"wrap_links",
			"hot_links_by_url_listing",
			"hot",
			"js.py",
			"reddit.js",
			"reddis.js",
			"build-module",
			"buildmodule"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 6,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "/D/Git/reddit/r2/r2/lib/normalized_hot.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3817,
						"regions":
						{
						},
						"selection":
						[
							[
								2831,
								2831
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2018.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "/D/Git/reddit/r2/r2/lib/menus.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 25179,
						"regions":
						{
						},
						"selection":
						[
							[
								2554,
								2554
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1352.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "/D/Git/reddit/r2/r2/controllers/wiki.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 22249,
						"regions":
						{
						},
						"selection":
						[
							[
								2323,
								2323
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1360.0,
						"zoom_level": 1.0
					},
					"stack_index": 9,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "/D/Git/reddit/r2/r2/lib/db/sorts.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1242,
						"regions":
						{
						},
						"selection":
						[
							[
								1124,
								1124
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 137.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "/D/Git/reddit/r2/r2/lib/db/_sorts.pyx",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4556,
						"regions":
						{
						},
						"selection":
						[
							[
								1694,
								1694
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 513.0,
						"zoom_level": 1.0
					},
					"stack_index": 10,
					"type": "text"
				},
				{
					"buffer": 5,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 79399,
						"regions":
						{
							"match":
							{
								"flags": 112,
								"regions":
								[
									[
										156,
										168
									],
									[
										398,
										410
									],
									[
										662,
										674
									],
									[
										937,
										949
									],
									[
										1179,
										1191
									],
									[
										1443,
										1455
									],
									[
										1848,
										1854
									],
									[
										2095,
										2101
									],
									[
										2516,
										2522
									],
									[
										2535,
										2541
									],
									[
										2859,
										2865
									],
									[
										2872,
										2878
									],
									[
										3091,
										3097
									],
									[
										3104,
										3110
									],
									[
										3402,
										3408
									],
									[
										3761,
										3767
									],
									[
										3779,
										3785
									],
									[
										3798,
										3804
									],
									[
										4055,
										4061
									],
									[
										4113,
										4119
									],
									[
										4270,
										4276
									],
									[
										4305,
										4311
									],
									[
										4399,
										4405
									],
									[
										4597,
										4603
									],
									[
										4702,
										4708
									],
									[
										4763,
										4769
									],
									[
										4877,
										4883
									],
									[
										4940,
										4946
									],
									[
										5123,
										5129
									],
									[
										5231,
										5237
									],
									[
										5530,
										5536
									],
									[
										5777,
										5783
									],
									[
										5988,
										5994
									],
									[
										5995,
										6001
									],
									[
										6153,
										6159
									],
									[
										6378,
										6384
									],
									[
										6396,
										6402
									],
									[
										6618,
										6624
									],
									[
										6789,
										6795
									],
									[
										7029,
										7035
									],
									[
										7052,
										7058
									],
									[
										7137,
										7143
									],
									[
										7452,
										7458
									],
									[
										7506,
										7512
									],
									[
										7607,
										7613
									],
									[
										7626,
										7632
									],
									[
										7662,
										7668
									],
									[
										7776,
										7782
									],
									[
										8036,
										8042
									],
									[
										8158,
										8164
									],
									[
										8227,
										8233
									],
									[
										8263,
										8269
									],
									[
										8272,
										8278
									],
									[
										8287,
										8293
									],
									[
										8363,
										8369
									],
									[
										8459,
										8465
									],
									[
										8580,
										8586
									],
									[
										8766,
										8772
									],
									[
										8878,
										8884
									],
									[
										8910,
										8916
									],
									[
										9011,
										9017
									],
									[
										9051,
										9057
									],
									[
										9210,
										9216
									],
									[
										9275,
										9281
									],
									[
										9378,
										9384
									],
									[
										9463,
										9469
									],
									[
										9511,
										9517
									],
									[
										9584,
										9590
									],
									[
										9633,
										9639
									],
									[
										9678,
										9684
									],
									[
										9764,
										9770
									],
									[
										9787,
										9793
									],
									[
										9804,
										9810
									],
									[
										9863,
										9869
									],
									[
										9885,
										9891
									],
									[
										9931,
										9937
									],
									[
										9965,
										9971
									],
									[
										9982,
										9988
									],
									[
										10074,
										10080
									],
									[
										10131,
										10137
									],
									[
										10249,
										10255
									],
									[
										10280,
										10286
									],
									[
										10350,
										10356
									],
									[
										10489,
										10495
									],
									[
										10625,
										10631
									],
									[
										10691,
										10697
									],
									[
										10946,
										10952
									],
									[
										11314,
										11320
									],
									[
										11652,
										11658
									],
									[
										11837,
										11843
									],
									[
										12075,
										12081
									],
									[
										12082,
										12088
									],
									[
										12285,
										12291
									],
									[
										12294,
										12300
									],
									[
										12478,
										12484
									],
									[
										12680,
										12686
									],
									[
										12760,
										12766
									],
									[
										12908,
										12914
									],
									[
										13030,
										13036
									],
									[
										13184,
										13190
									],
									[
										13354,
										13360
									],
									[
										13537,
										13543
									],
									[
										13699,
										13705
									],
									[
										14065,
										14071
									],
									[
										14474,
										14480
									],
									[
										14515,
										14521
									],
									[
										14734,
										14740
									],
									[
										14887,
										14893
									],
									[
										14895,
										14901
									],
									[
										14906,
										14912
									],
									[
										15140,
										15146
									],
									[
										15772,
										15778
									],
									[
										16216,
										16222
									],
									[
										16657,
										16663
									],
									[
										17075,
										17081
									],
									[
										17658,
										17664
									],
									[
										18160,
										18166
									],
									[
										18670,
										18676
									],
									[
										19177,
										19183
									],
									[
										19676,
										19682
									],
									[
										20183,
										20189
									],
									[
										20698,
										20704
									],
									[
										21205,
										21211
									],
									[
										21720,
										21726
									],
									[
										22210,
										22216
									],
									[
										22704,
										22710
									],
									[
										23189,
										23195
									],
									[
										23675,
										23681
									],
									[
										24061,
										24067
									],
									[
										24511,
										24517
									],
									[
										25171,
										25177
									],
									[
										25627,
										25633
									],
									[
										25891,
										25897
									],
									[
										26421,
										26438
									],
									[
										26819,
										26825
									],
									[
										27066,
										27072
									],
									[
										27487,
										27493
									],
									[
										27506,
										27512
									],
									[
										27830,
										27836
									],
									[
										27843,
										27849
									],
									[
										28062,
										28068
									],
									[
										28075,
										28081
									],
									[
										28373,
										28379
									],
									[
										28732,
										28738
									],
									[
										28750,
										28756
									],
									[
										28769,
										28775
									],
									[
										29026,
										29032
									],
									[
										29084,
										29090
									],
									[
										29241,
										29247
									],
									[
										29276,
										29282
									],
									[
										29370,
										29376
									],
									[
										29568,
										29574
									],
									[
										29673,
										29679
									],
									[
										29734,
										29740
									],
									[
										29848,
										29854
									],
									[
										29911,
										29917
									],
									[
										30094,
										30100
									],
									[
										30202,
										30208
									],
									[
										30501,
										30507
									],
									[
										30748,
										30754
									],
									[
										30959,
										30965
									],
									[
										30966,
										30972
									],
									[
										31124,
										31130
									],
									[
										31349,
										31355
									],
									[
										31367,
										31373
									],
									[
										31589,
										31595
									],
									[
										31760,
										31766
									],
									[
										32000,
										32006
									],
									[
										32023,
										32029
									],
									[
										32108,
										32114
									],
									[
										32423,
										32429
									],
									[
										32477,
										32483
									],
									[
										32578,
										32584
									],
									[
										32597,
										32603
									],
									[
										32633,
										32639
									],
									[
										32747,
										32753
									],
									[
										33007,
										33013
									],
									[
										33129,
										33135
									],
									[
										33198,
										33204
									],
									[
										33234,
										33240
									],
									[
										33243,
										33249
									],
									[
										33258,
										33264
									],
									[
										33334,
										33340
									],
									[
										33430,
										33436
									],
									[
										33551,
										33557
									],
									[
										33737,
										33743
									],
									[
										33849,
										33855
									],
									[
										33881,
										33887
									],
									[
										33982,
										33988
									],
									[
										34022,
										34028
									],
									[
										34181,
										34187
									],
									[
										34246,
										34252
									],
									[
										34349,
										34355
									],
									[
										34434,
										34440
									],
									[
										34482,
										34488
									],
									[
										34555,
										34561
									],
									[
										34604,
										34610
									],
									[
										34649,
										34655
									],
									[
										34735,
										34741
									],
									[
										34758,
										34764
									],
									[
										34775,
										34781
									],
									[
										34834,
										34840
									],
									[
										34856,
										34862
									],
									[
										34902,
										34908
									],
									[
										34936,
										34942
									],
									[
										34953,
										34959
									],
									[
										35045,
										35051
									],
									[
										35102,
										35108
									],
									[
										35220,
										35226
									],
									[
										35251,
										35257
									],
									[
										35321,
										35327
									],
									[
										35460,
										35466
									],
									[
										35596,
										35602
									],
									[
										35662,
										35668
									],
									[
										35917,
										35923
									],
									[
										36285,
										36291
									],
									[
										36623,
										36629
									],
									[
										36808,
										36814
									],
									[
										37046,
										37052
									],
									[
										37053,
										37059
									],
									[
										37256,
										37262
									],
									[
										37265,
										37271
									],
									[
										37449,
										37455
									],
									[
										37651,
										37657
									],
									[
										37731,
										37737
									],
									[
										37879,
										37885
									],
									[
										38001,
										38007
									],
									[
										38155,
										38161
									],
									[
										38325,
										38331
									],
									[
										38508,
										38514
									],
									[
										38670,
										38676
									],
									[
										39036,
										39042
									],
									[
										39445,
										39451
									],
									[
										39486,
										39492
									],
									[
										39705,
										39711
									],
									[
										39858,
										39864
									],
									[
										39866,
										39872
									],
									[
										39877,
										39883
									],
									[
										40111,
										40117
									],
									[
										40743,
										40749
									],
									[
										41187,
										41193
									],
									[
										41628,
										41634
									],
									[
										42046,
										42052
									],
									[
										42629,
										42635
									],
									[
										43131,
										43137
									],
									[
										43641,
										43647
									],
									[
										44148,
										44154
									],
									[
										44647,
										44653
									],
									[
										45154,
										45160
									],
									[
										45669,
										45675
									],
									[
										46176,
										46182
									],
									[
										46691,
										46697
									],
									[
										47181,
										47187
									],
									[
										47675,
										47681
									],
									[
										48160,
										48166
									],
									[
										48646,
										48652
									],
									[
										49032,
										49038
									],
									[
										49482,
										49488
									],
									[
										50142,
										50148
									],
									[
										50598,
										50604
									],
									[
										50862,
										50868
									],
									[
										51225,
										51236
									],
									[
										51370,
										51381
									],
									[
										51543,
										51554
									],
									[
										51688,
										51699
									],
									[
										51860,
										51870
									],
									[
										52106,
										52116
									],
									[
										52293,
										52304
									],
									[
										52438,
										52449
									],
									[
										52778,
										52784
									],
									[
										53025,
										53031
									],
									[
										53446,
										53452
									],
									[
										53465,
										53471
									],
									[
										53789,
										53795
									],
									[
										53802,
										53808
									],
									[
										54021,
										54027
									],
									[
										54034,
										54040
									],
									[
										54332,
										54338
									],
									[
										54691,
										54697
									],
									[
										54709,
										54715
									],
									[
										54728,
										54734
									],
									[
										54985,
										54991
									],
									[
										55043,
										55049
									],
									[
										55200,
										55206
									],
									[
										55235,
										55241
									],
									[
										55329,
										55335
									],
									[
										55527,
										55533
									],
									[
										55632,
										55638
									],
									[
										55693,
										55699
									],
									[
										55807,
										55813
									],
									[
										55870,
										55876
									],
									[
										56053,
										56059
									],
									[
										56161,
										56167
									],
									[
										56460,
										56466
									],
									[
										56707,
										56713
									],
									[
										56918,
										56924
									],
									[
										56925,
										56931
									],
									[
										57083,
										57089
									],
									[
										57308,
										57314
									],
									[
										57326,
										57332
									],
									[
										57548,
										57554
									],
									[
										57719,
										57725
									],
									[
										57959,
										57965
									],
									[
										57982,
										57988
									],
									[
										58067,
										58073
									],
									[
										58382,
										58388
									],
									[
										58436,
										58442
									],
									[
										58537,
										58543
									],
									[
										58556,
										58562
									],
									[
										58592,
										58598
									],
									[
										58706,
										58712
									],
									[
										58966,
										58972
									],
									[
										59088,
										59094
									],
									[
										59157,
										59163
									],
									[
										59193,
										59199
									],
									[
										59202,
										59208
									],
									[
										59217,
										59223
									],
									[
										59293,
										59299
									],
									[
										59389,
										59395
									],
									[
										59510,
										59516
									],
									[
										59696,
										59702
									],
									[
										59808,
										59814
									],
									[
										59840,
										59846
									],
									[
										59941,
										59947
									],
									[
										59981,
										59987
									],
									[
										60140,
										60146
									],
									[
										60205,
										60211
									],
									[
										60308,
										60314
									],
									[
										60393,
										60399
									],
									[
										60441,
										60447
									],
									[
										60514,
										60520
									],
									[
										60563,
										60569
									],
									[
										60608,
										60614
									],
									[
										60694,
										60700
									],
									[
										60717,
										60723
									],
									[
										60734,
										60740
									],
									[
										60793,
										60799
									],
									[
										60815,
										60821
									],
									[
										60861,
										60867
									],
									[
										60895,
										60901
									],
									[
										60912,
										60918
									],
									[
										61004,
										61010
									],
									[
										61061,
										61067
									],
									[
										61179,
										61185
									],
									[
										61210,
										61216
									],
									[
										61280,
										61286
									],
									[
										61419,
										61425
									],
									[
										61555,
										61561
									],
									[
										61621,
										61627
									],
									[
										61876,
										61882
									],
									[
										62244,
										62250
									],
									[
										62582,
										62588
									],
									[
										62767,
										62773
									],
									[
										63005,
										63011
									],
									[
										63012,
										63018
									],
									[
										63215,
										63221
									],
									[
										63224,
										63230
									],
									[
										63408,
										63414
									],
									[
										63610,
										63616
									],
									[
										63690,
										63696
									],
									[
										63838,
										63844
									],
									[
										63960,
										63966
									],
									[
										64114,
										64120
									],
									[
										64284,
										64290
									],
									[
										64467,
										64473
									],
									[
										64629,
										64635
									],
									[
										64995,
										65001
									],
									[
										65404,
										65410
									],
									[
										65445,
										65451
									],
									[
										65664,
										65670
									],
									[
										65817,
										65823
									],
									[
										65825,
										65831
									],
									[
										65836,
										65842
									],
									[
										66070,
										66076
									],
									[
										66702,
										66708
									],
									[
										67146,
										67152
									],
									[
										67587,
										67593
									],
									[
										68005,
										68011
									],
									[
										68588,
										68594
									],
									[
										69090,
										69096
									],
									[
										69600,
										69606
									],
									[
										70107,
										70113
									],
									[
										70606,
										70612
									],
									[
										71113,
										71119
									],
									[
										71628,
										71634
									],
									[
										72135,
										72141
									],
									[
										72650,
										72656
									],
									[
										73140,
										73146
									],
									[
										73634,
										73640
									],
									[
										74119,
										74125
									],
									[
										74605,
										74611
									],
									[
										74991,
										74997
									],
									[
										75441,
										75447
									],
									[
										76101,
										76107
									],
									[
										76557,
										76563
									],
									[
										76821,
										76827
									],
									[
										77379,
										77396
									],
									[
										77638,
										77655
									],
									[
										77743,
										77760
									],
									[
										77893,
										77910
									],
									[
										78095,
										78112
									],
									[
										78547,
										78564
									],
									[
										78806,
										78823
									],
									[
										78911,
										78928
									],
									[
										79061,
										79078
									],
									[
										79263,
										79280
									]
								],
								"scope": ""
							}
						},
						"selection":
						[
							[
								79264,
								79264
							]
						],
						"settings":
						{
							"detect_indentation": false,
							"line_numbers": false,
							"output_tag": 12,
							"result_base_dir": "",
							"result_file_regex": "^([A-Za-z\\\\/<].*):$",
							"result_line_regex": "^ +([0-9]+):",
							"scroll_past_end": true,
							"syntax": "Packages/Default/Find Results.hidden-tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 47655.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "/D/Git/reddit/r2/r2/lib/rising.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2889,
						"regions":
						{
						},
						"selection":
						[
							[
								2654,
								2654
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2000.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "/D/Git/reddit/r2/r2/lib/count.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1793,
						"regions":
						{
						},
						"selection":
						[
							[
								1456,
								1456
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage"
						},
						"translation.x": -0.0,
						"translation.y": 567.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "/D/Git/reddit/r2/r2/lib/db/tdb_sql.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 33941,
						"regions":
						{
						},
						"selection":
						[
							[
								4331,
								4331
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3323.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "/D/Git/reddit/r2/r2/lib/db/tdb_lite.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2927,
						"regions":
						{
						},
						"selection":
						[
							[
								1092,
								1092
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 245.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "/D/Git/reddit/r2/r2/controllers/listingcontroller.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 66022,
						"regions":
						{
						},
						"selection":
						[
							[
								19415,
								19373
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 13933.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "/D/Git/reddit/r2/r2/lib/app_globals.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 34447,
						"regions":
						{
						},
						"selection":
						[
							[
								3862,
								3862
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1701.0,
						"zoom_level": 1.0
					},
					"stack_index": 14,
					"type": "text"
				},
				{
					"buffer": 12,
					"file": "src/packages/repositories.config",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 873,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/XML/XML.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 13,
					"type": "text"
				},
				{
					"buffer": 13,
					"file": "/D/Git/reddit/r2/r2/lib/eventcollector.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11465,
						"regions":
						{
						},
						"selection":
						[
							[
								3819,
								3819
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2216.0,
						"zoom_level": 1.0
					},
					"stack_index": 21,
					"type": "text"
				},
				{
					"buffer": 14,
					"file": "/D/Git/reddit/r2/r2/models/listing.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11808,
						"regions":
						{
						},
						"selection":
						[
							[
								11619,
								11619
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 9506.0,
						"zoom_level": 1.0
					},
					"stack_index": 16,
					"type": "text"
				},
				{
					"buffer": 15,
					"file": "/D/Git/reddit/r2/example.ini",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 29821,
						"regions":
						{
						},
						"selection":
						[
							[
								4101,
								4101
							]
						],
						"settings":
						{
							"syntax": "Packages/EditorConfig/INI.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 2596.0,
						"zoom_level": 1.0
					},
					"stack_index": 12,
					"type": "text"
				},
				{
					"buffer": 16,
					"file": "/D/Git/reddit/r2/r2/lib/organic.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2739,
						"regions":
						{
						},
						"selection":
						[
							[
								1875,
								1875
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 839.0,
						"zoom_level": 1.0
					},
					"stack_index": 17,
					"type": "text"
				},
				{
					"buffer": 17,
					"file": "/D/Git/reddit/r2/r2/models/link.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 93535,
						"regions":
						{
						},
						"selection":
						[
							[
								2809,
								2814
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1130.0,
						"zoom_level": 1.0
					},
					"stack_index": 15,
					"type": "text"
				},
				{
					"buffer": 18,
					"file": "/D/Git/reddit/r2/r2/models/query_cache.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 21630,
						"regions":
						{
						},
						"selection":
						[
							[
								16934,
								16934
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 13550.0,
						"zoom_level": 1.0
					},
					"stack_index": 18,
					"type": "text"
				},
				{
					"buffer": 19,
					"file": "/D/Git/reddit/r2/r2/lib/db/queries.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 67915,
						"regions":
						{
						},
						"selection":
						[
							[
								12560,
								12560
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 8795.0,
						"zoom_level": 1.0
					},
					"stack_index": 11,
					"type": "text"
				},
				{
					"buffer": 20,
					"file": "/D/Git/reddit/r2/r2/lib/pages/things.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 14006,
						"regions":
						{
						},
						"selection":
						[
							[
								13288,
								13288
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 6395.0,
						"zoom_level": 1.0
					},
					"stack_index": 20,
					"type": "text"
				},
				{
					"buffer": 21,
					"file": "/D/Git/reddit/r2/r2/controllers/api.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 177066,
						"regions":
						{
						},
						"selection":
						[
							[
								150537,
								150542
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 103973.0,
						"zoom_level": 1.0
					},
					"stack_index": 19,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 35.0
	},
	"input":
	{
		"height": 0.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "Skimur.sublime-project",
	"replace":
	{
		"height": 66.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"config.h",
				"ffserver_config.h"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 253.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
